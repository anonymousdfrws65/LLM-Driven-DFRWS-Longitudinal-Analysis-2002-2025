{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba4d77-79c4-46b6-a09a-7212abdea3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d4458-da72-4a59-99fb-12ec76f88f55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import PyPDF2\n",
    "\n",
    "# Function to extract PDFs from a folder\n",
    "def extract_pdfs_from_folder(pdf_folder):\n",
    "    pdf_files = []\n",
    "    for file_name in os.listdir(pdf_folder):\n",
    "        if file_name.endswith('.pdf'):\n",
    "            pdf_files.append(os.path.join(pdf_folder, file_name))\n",
    "    return pdf_files\n",
    "\n",
    "# Robust title extractor with multi-line handling and city/date filtering\n",
    "def extract_title_from_pdf(pdf_text):\n",
    "    lines = pdf_text.splitlines()\n",
    "\n",
    "    skip_patterns = [\n",
    "        r'DFRWS.*\\d{4}',\n",
    "        r'Digital Investigation',\n",
    "        r'Forensic Science International',\n",
    "        r'ScienceDirect',\n",
    "        r'ELSEVIER',\n",
    "        r'CrossMark',\n",
    "        r'Contents lists available at',\n",
    "        r'journal homepage',\n",
    "        r'Available at',\n",
    "        r'www\\.',\n",
    "        r'^$',\n",
    "        r'^DIGITAL FORENSIC RESEARCH CONFERENCE$',\n",
    "        r'^DOI[:\\s]',\n",
    "        r'^http[s]?://',\n",
    "        r'creativecommons.org',\n",
    "    ]\n",
    "    bad_title_patterns = [\n",
    "        r'^\\(?[A-Z][a-z]+, [A-Z]{2} \\(',       # City, State ( e.g., Syracuse, NY (\n",
    "        r'^\\(?[A-Z][a-z]+ \\d{1,2}(st|nd|rd|th)?\\)?',  # Aug 6th\n",
    "        r'^\\(?\\d{4}\\)?$',                      # year\n",
    "    ]\n",
    "    stop_patterns = [\n",
    "        r'^By ',\n",
    "        r'From the proceedings of',\n",
    "        r'DFRWS \\d{4}',\n",
    "        r'\\(\\w{3,9} \\d{1,2}',\n",
    "        r'^\\d{4} \\d{1,2}:\\d{2}',               # timestamps\n",
    "        r'^Elsevier',\n",
    "        r'^Keywords',\n",
    "    ]\n",
    "\n",
    "    clean_lines = [line.strip() for line in lines if line.strip()]\n",
    "    title_candidates = []\n",
    "\n",
    "    for i in range(min(50, len(clean_lines))):\n",
    "        line = clean_lines[i]\n",
    "        if any(re.search(pat, line, re.IGNORECASE) for pat in skip_patterns):\n",
    "            continue\n",
    "        if any(re.search(pat, line.strip(), re.IGNORECASE) for pat in bad_title_patterns):\n",
    "            continue\n",
    "        if any(re.search(pat, line, re.IGNORECASE) for pat in stop_patterns):\n",
    "            break\n",
    "        if len(line.split()) < 2:\n",
    "            continue\n",
    "        title_candidates.append((i, line))\n",
    "\n",
    "    # Step 1: Multi-line title merge\n",
    "    for i, line in title_candidates:\n",
    "        if i + 1 < len(clean_lines):\n",
    "            next_line = clean_lines[i + 1].strip()\n",
    "            full_title = f\"{line} {next_line}\"\n",
    "            if 6 <= len(full_title.split()) <= 20 and not full_title.endswith('.'):\n",
    "                return full_title\n",
    "\n",
    "    # Step 2: Title right before \"By\"\n",
    "    for idx, line in title_candidates:\n",
    "        if idx + 1 < len(clean_lines) and clean_lines[idx + 1].lower().startswith(\"by \"):\n",
    "            return line\n",
    "\n",
    "    # Step 3: Fallback to any decent candidate\n",
    "    for _, line in title_candidates:\n",
    "        if len(line.split()) >= 4:\n",
    "            return line\n",
    "\n",
    "    return \"Unknown Title\"\n",
    "\n",
    "# Function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    try:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        pdf_text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page_text = pdf_reader.pages[page_num].extract_text()\n",
    "            if page_text:\n",
    "                pdf_text += page_text\n",
    "        return pdf_text\n",
    "    except KeyError as e:\n",
    "        print(f\"Error processing {pdf_file}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error processing {pdf_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main script\n",
    "pdf_folder = r\"full_dataset\"\n",
    "output_path = 'extracted_test_papers_new_dfrws.jsonl'\n",
    "\n",
    "papers = []\n",
    "pdf_files = extract_pdfs_from_folder(pdf_folder)\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    text = extract_text_from_pdf(pdf_file)\n",
    "    if text:\n",
    "        title = extract_title_from_pdf(text)\n",
    "        papers.append({\"title\": title, \"content\": text})\n",
    "        print(f\" {os.path.basename(pdf_file)} → Title: {title}\")\n",
    "    else:\n",
    "        print(f\" Failed to extract text from: {os.path.basename(pdf_file)}\")\n",
    "\n",
    "# Save output to JSONL\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for paper in papers:\n",
    "        safe_text = json.dumps(paper, ensure_ascii=False).encode('utf-8', 'ignore').decode('utf-8')\n",
    "        f.write(safe_text + \"\\n\")\n",
    "\n",
    "print(f\"\\n Extraction complete. Output saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bde52e-efdb-4be8-9271-068b8c5fc4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78137715-56db-4b0d-8418-aa7f0a027040",
   "metadata": {},
   "source": [
    "Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be92ae48-193b-49b3-a4d6-81dbce3e788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_title_prompt(paper):\n",
    "    title = paper['title']\n",
    "    content = paper['content']\n",
    "    return f'''\n",
    "    You are tasked with extracting the full title from the digital forensics paper titled \"{title}\".\n",
    "\n",
    "    Guidelines:\n",
    "    - The title is usually at the top of the first page or in the first section.\n",
    "    - Extract the title in its entirety.\n",
    "\n",
    "    Your response must be in the following JSON format:\n",
    "    {{\n",
    "        \"title\": \"Title of the paper here\"\n",
    "    }}\n",
    "\n",
    "    Here is the paper content:\n",
    "    <Start of Paper Content>\n",
    "    {content}\n",
    "    <End of Paper Content>\n",
    "\n",
    "    Your response: \"\"\"\n",
    "    '''\n",
    "\n",
    "\n",
    "def generate_tools_prompt(paper):\n",
    "    title = paper['title']\n",
    "    content = paper['content']\n",
    "    \n",
    "    return f'''\n",
    "    You are tasked with extracting **tools** mentioned in the digital forensics paper titled \"{title}\".\n",
    "\n",
    "   Guidelines:\n",
    "    \\t1. A **tool** is any named software, framework, script, or system used or created for forensic or anti-forensic purposes.\n",
    "    \\t- Only include a tool if it is actually used, created, or extended in the paper. \n",
    "    \\t- If a tool is **referenced** or just **mentioned** for context and NOT used, do NOT include it in the tools output. For instance, referred in  \n",
    "    sections like **Related Work** or **Literature Review** section does not count.\n",
    "\n",
    "    \\t2. For each tool actually **used**, **created**, or **extended**, provide:\n",
    "       \\t- \"tool_name\"\n",
    "       \\t- \"action\": \"used\" or \"created\" or \"extended\"\n",
    "       \\t- \"repository_link\" (URL or empty string)\n",
    "       \\t- \"license\": must be one of:\n",
    "          \\t\\t- \"open-source\": source code is publicly available, allowing others to inspect, modify, and extend. Open-source tools are continuously updated and widely reused. They enable creativity and expansion since others can build upon the original code. Examples: Kali Linux, CAINE, Autopsy.\n",
    "          \\t\\t- \"proprietary\": source code is closed and controlled by the originator (e.g., company or vendor). Only the developer can modify or distribute it. Proprietary tools are widely used in practice, especially in law enforcement, but cannot be extended by the community. Examples: FTK Forensic Toolkit, FTK Imager, Magnet AXIOM (Magnet Forensics).\n",
    "          \\t\\t- \"not-mentioned\": if the license type is not explicitly stated and no reliable source (e.g., URL, DOI) is available.\n",
    "       \\t- \"origin\": one of:\n",
    "          \\t\\t- \"academic_research_DFRWS\" if the tool was first introduced by this DFRWS paper\n",
    "          \\t\\t- \"academic_research_external\" if the tool was created in other academic venues (conferences, journals, academic projects)\n",
    "          \\t\\t- \"organization\" if the tool was created by companies, vendors, or non-academic organizations\n",
    "          \\t\\t- \"not-mentioned\" if the origin is not explicitly stated\n",
    "\n",
    "    \\t3. Special case:\n",
    "       \\t- If the paper introduces a plugin, module, extension, or significant modification of an existing tool, mark \"action\" as \"extended\" and apply the same origin rules.\n",
    "       \\t- Also list the base tool separately if it was explicitly used.\n",
    "\n",
    "    \\t4. If the paper uses **no tools**, return: null\n",
    "\n",
    "       {{\n",
    "         \"tools\": [{{\"tool_name\": null, \"action\": null, \"repository_link\": null, \"repository_type\": null, \"authorship\": null}}]\n",
    "       }}\n",
    "\n",
    "    \\t- Null Cases Example 1: In the paper \"How to Reuse Knowledge about Forensic Investigations, the authors didnt used/created/extended any tool, so the output should be \"null\" in all fields.\n",
    "\n",
    "    {{\n",
    "         \"tools\": [{{\"tool_name\": null, \"action\": null, \"repository_link\": null, \"repository_type\": null, \"authorship\": null}}]\n",
    "    }}\n",
    "\n",
    "    \\t- Null Cases Example 2: In the paper \"A survey of forensic characterization methods for physical devices\", they author didnt used/created/extended since its a survey paper, so the output should be:\n",
    "    \n",
    "    {{\n",
    "         \"tools\": [{{\"tool_name\": null, \"action\": null, \"repository_link\": null, \"repository_type\": null, \"authorship\": null}}]\n",
    "    }}\n",
    "    \n",
    "    \\t5. If a tool is found but any field (action, repository_link, repository_type, or authorship) is not mentioned, assign \"not-mentioned\" to that field.\n",
    "\n",
    "    JSON Format:\n",
    "    {{\n",
    "      \"tools\": [\n",
    "        {{\n",
    "          \"tool_name\": \"Volatility\",\n",
    "          \"action\": \"used\",\n",
    "          \"repository_link\": \"https://github.com/volatilityfoundation/volatility\",\n",
    "          \"repository_type\": \"open-source\",\n",
    "          \"origin\": \"organization\"\n",
    "        }},\n",
    "    \n",
    "        {{\n",
    "          \"tool_name\":\"DROP\",\n",
    "          \"action\": \"created\"\n",
    "          \"repository_link\": \"https://github.com/unhcfreg/DROP\",\n",
    "          \"repository_type\": \"open-source\",\n",
    "          \"origin\": \"academic_research_DFRWS\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "    \\t. For example in the paper \"So fresh, so clean: Cloud forensic analysis of the Amazon iRobot Roomba vacuum\", DFRWS authors created\n",
    "    a tool and made it open-source:\n",
    "    {{\n",
    "      \"tools\": [\n",
    "        {{\n",
    "          \"tool_name\": \"PyRoomba\",\n",
    "          \"action\": \"created\",\n",
    "          \"repository_link\": \"https://github.com/BiTLab-BaggiliTruthLab/PyRoomba\",\n",
    "          \"repository_type\": \"open-source\",\n",
    "          \"origin\": \"academic_research_DFRWS\"\n",
    "        }}\n",
    "    }}\n",
    "    \n",
    "    Here is the paper content: \n",
    "    <Start of Paper Content>\n",
    "    {content}\n",
    "    <End of Paper Content>\n",
    "\n",
    "    Your response: \"\"\"\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e1ebc-a252-4345-8f83-1b8c5bb616ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env\n",
    "load_dotenv(\"api_key.env\")\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Your prompt generators must be defined above or in the same file:\n",
    "# def generate_title_prompt(paper): …\n",
    "# def generate_tools_prompt(paper): …\n",
    "\n",
    "# Your processor\n",
    "def process_papers_for_tasks(papers, tasks):\n",
    "    task_results = {}\n",
    "    for i, paper in enumerate(papers):\n",
    "        paper_title = paper['title']\n",
    "        print(f\"\\nProcessing paper: {paper_title}\")\n",
    "        task_results[paper_title] = {}\n",
    "\n",
    "        for task in tasks:\n",
    "            if task == \"title\":\n",
    "                user_prompt = generate_title_prompt(paper)\n",
    "            elif task == \"tools\":\n",
    "                user_prompt = generate_tools_prompt(paper)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model='gpt-4o-mini',\n",
    "                    messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "                    temperature=0.2,\n",
    "                    max_tokens=5000\n",
    "                )\n",
    "                response_text = response.choices[0].message.content\n",
    "                print(response_text)\n",
    "                task_results[paper_title][task] = response_text\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {task} for paper {i+1}: {e}\")\n",
    "                task_results[paper_title][task] = f\"error: {e}\"\n",
    "\n",
    "    return task_results\n",
    "\n",
    "\n",
    "# Run on all papers\n",
    "\n",
    "test_papers = papers  # assumes 'papers' is a list of dicts with 'title' and 'content'\n",
    "tasks = [\"title\", \"tools\"]\n",
    "\n",
    "results = process_papers_for_tasks(test_papers, tasks)\n",
    "\n",
    "# Save CSV\n",
    "with open(\"results_new_last_tools.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Paper Title\"] + tasks)\n",
    "    for paper_title, r in results.items():\n",
    "        row = [paper_title] + [r.get(task, \"No result\") for task in tasks]\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Results saved to results_new_last_tools.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f210f29-d920-4593-8936-1d3d197e18d2",
   "metadata": {},
   "source": [
    "Metadata Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea10d6b9-8302-4461-ba0a-e2410cb54261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_metadata_prompt(task, paper, ontology_json=None):\n",
    "    title = paper['title']\n",
    "    content = paper['content']\n",
    "\n",
    "    if task == \"title\":\n",
    "        return f'''\n",
    "        You are tasked with extracting the full title from the digital forensics paper titled \"{title}\".\n",
    "\n",
    "        Guidelines:\n",
    "        - The title is usually at the top of the first page or in the first section.\n",
    "        - Extract the title in its entirety.\n",
    "\n",
    "        Your response must be in the following JSON format:\n",
    "        {{\n",
    "            \"title\": \"Title of the paper here\"\n",
    "        }}\n",
    "\n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your response: \"\"\"\n",
    "        '''\n",
    "\n",
    "    elif task == \"authors\":\n",
    "        return f'''\n",
    "        You are tasked with extracting the list of authors from the paper titled \"{title}\".\n",
    "\n",
    "        Guidelines:\n",
    "        - The authors' names are typically listed directly below the title or in the header/footer.\n",
    "        - Extract all authors, separated by commas.\n",
    "\n",
    "        Your response must be in the following JSON format:\n",
    "        {{\n",
    "            \"authors\": \"Comma-separated list of authors' names here\"\n",
    "        }}\n",
    "\n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your response: \"\"\"\n",
    "        '''\n",
    "    elif task == \"school_names\":\n",
    "        return f'''\n",
    "        You are tasked with extracting the name(s) of academic institutions or organizations affiliated with the authors of the paper titled \"{title}\".\n",
    "\n",
    "        Guidelines:\n",
    "        - The school or institutional affiliations are usually listed below the authors' names or in the first page.\n",
    "        - Extract **all** school names or affiliations mentioned.\n",
    "        - Include universities, colleges, research institutions, or companies if provided.\n",
    "        - Remove duplicates if the same institution is mentioned multiple times.\n",
    "\n",
    "        Your response must be returned in the following JSON format:\n",
    "        {{\n",
    "            \"school_names\": [\"University of California, Berkeley\", \"National University of Singapore\"]\n",
    "        }}\n",
    "\n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your response: \"\"\"\n",
    "        '''\n",
    "    elif task == \"author_countries\":\n",
    "        return f'''\n",
    "        You are tasked with extracting the country or countries where the authors or their affiliated institutions are based for the paper titled \"{title}\".\n",
    "\n",
    "        Guidelines:\n",
    "        - Country names are often listed after the institution name, or appear in the corresponding author information, address section, or footnotes.\n",
    "        - Only look at the countries mentioned alongside author names, typically found at the top of the paper. Do not scan the entire document.\n",
    "        - If a country is not explicitly stated but can be reliably inferred from a well-known institution (e.g., MIT → USA), include it.\n",
    "        - Do not guess or hallucinate. If the country cannot be determined, return \"null\".\n",
    "\n",
    "        Your response must be in the following JSON format:\n",
    "        {{\n",
    "            \"author_countries\": [\"USA\", \"Germany\"]\n",
    "        }}\n",
    "\n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your response: \"\"\"\n",
    "        '''\n",
    "\n",
    "\n",
    "    elif task == \"conference\":\n",
    "        return f'''\n",
    "        \n",
    "        You are tasked with identifying the conference of publication for the paper titled \"{title}\".\n",
    "\n",
    "        Guidelines:\n",
    "        \\t1- The conference name will always be one of the following:\n",
    "        \\t- \"DFRWS USA\"\n",
    "        \\t- \"DFRWS Europe\"\n",
    "        \\t- \"DFRWS APAC\"\n",
    "        \n",
    "        \\t2- Read the paper carefully to determine which DFRWS conference it was published in.\n",
    "        \n",
    "        Your response must be in the following JSON format:\n",
    "        {{\n",
    "            \"conference\": \"DFRWS USA\"\n",
    "        }}\n",
    "\n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your response: \"\"\"\n",
    "        '''\n",
    "\n",
    "    elif task == \"published_year\":\n",
    "        return f'''\n",
    "        You are tasked with extracting the year of publication from the digital forensics paper titled \"{title}\".\n",
    "\n",
    "        Guidelines:\n",
    "        - The year is usually found near the conference name or in the paper's footer/header.\n",
    "\n",
    "        Your response must be in the following JSON format:\n",
    "        {{\n",
    "            \"year\": \"Year of publication here\"\n",
    "        }}\n",
    "\n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your response: \"\"\"\n",
    "        '''\n",
    "    elif task == \"forensic_vs_anti\":\n",
    "        return f'''\n",
    "       \n",
    "       You are tasked with extracting the following metadata from the provided digital forensics paper titled \"{title}\":\n",
    "       \n",
    "       You must classify the paper as either **Forensic** or **Anti-Forensic**.\n",
    "\n",
    "        Definition for Identifying paper type:\n",
    "        \\t1- **Digital Forensic**: A paper that supports, enhances, or proposes solutions for conducting digital forensic investigations.\n",
    "        \\t2- **Anti-Forensic**: A paper that explores techniques or tools meant to subvert, obscure, or defeat digital forensic processes, often discussed either to develop countermeasures or expose threats.\n",
    "\n",
    "        Book Definitions:\n",
    "        \\t1- **Digital Forensics**: The application of science to the identification, collection/acquisition, examination, and analysis of data while preserving the integrity of the information and maintaining a strict chain \n",
    "        of custody for the data.\n",
    "        \\t2- **Anti-Forensic**: A technique for concealing or destroying data so that others cannot access it.\n",
    "\n",
    "        Guidelines:\n",
    "        \\t1- Determine whether the focus of the paper aligns more with digital forensic goals (acquisition, analysis, etc.) or with undermining such goals.\n",
    "        \\t2- You must assess the paper’s core contribution and stance. Just because a paper discusses anti-forensic techniques does **not** necessarily mean it is an anti-forensic paper.\n",
    "        \\t3- If a paper studies anti-forensic techniques with the goal of developing better forensic tools, defenses, or acquisition methods, it is still considered ** Digital Forensic**.\n",
    "        \\t4- If a paper introduces or promotes anti-forensic techniques without a forensic enhancement objective, then it is **Anti-Forensic**.\n",
    "\n",
    "\n",
    "        Your response must be in the following JSON format:\n",
    "        {{\n",
    "            \"forensic_type\": \"Digital Forensic\"  // or \"Anti-Forensic\"\n",
    "        }}\n",
    "    \n",
    "    \n",
    "        \\t- Examples in JSON format.\n",
    "        Example 1:\n",
    "        In the paper \"Anti-forensic resilient memory acquisition\", the authors design a memory acquisition technique that is robust against anti-forensic attacks.\n",
    "        Since the paper contributes to strengthening digital forensic methods and improving resilience, it should be classified as forensic.\n",
    "        {{\n",
    "            \"forensic_type\": \"Forensic\"\n",
    "        }}\n",
    "\n",
    "        Example 2:\n",
    "        In the paper \"Android anti-forensics through a local paradigm\", the authors present practical techniques to undermine forensic processes on Android devices, including delaying and manipulating evidence. \n",
    "        These methods aim to defeat forensic tools, making the paper anti-forensic.\n",
    "        {{\n",
    "            \"forensic_type\": \"Anti-Forensic\"\n",
    "        }}\n",
    "\n",
    "        Example 3:\n",
    "        In the paper \"Forensic carving of network packets and associated data structures\" (2011), the focus is on developing techniques for reconstructing network packets from raw data to aid in forensic investigations. \n",
    "        It enhances forensic analysis and does not attempt to conceal or disrupt it. Thus, it is classified as:\n",
    "        {{\n",
    "            \"forensic_type\": \"Forensic\"\n",
    "        }}\n",
    "\n",
    "\n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your response: \"\"\"\n",
    "        '''\n",
    "    \n",
    "    elif task == \"ontology_classification\":\n",
    "            return f'''\n",
    "            \n",
    "            You are tasked with classifying the research paper titled \"{title}\" based on a structured digital forensics ontology.\n",
    "            Guidelines:\n",
    "            \\t1- Choose only one value for each of the following fields: discipline, subdiscipline, object, subobject.\n",
    "            \\t2- If no match is found, return \"unknown\" for that field.\n",
    "            \\t3- If a new domain or object/subobject not in the list is clearly found, return it directly.\n",
    "            \\t4- You MUST choose only from the taxonomy structure below:\n",
    "\n",
    "            {{\n",
    "                \"Computer Forensics\": {{\n",
    "                \"Server Forensics\": {{\"Hard Disk\": [\"Logs\"], \"Registers\": [], \"RAM\": []}},\n",
    "                \"Laptop Forensics\": {{\"Hard Disk\": [\"Logs\"], \"Registers\": [], \"RAM\": []}},\n",
    "                \"Desktop Forensics\": {{\"Hard Disk\": [\"Logs\"], \"Registers\": [], \"RAM\": []}}\n",
    "            }},\n",
    "                \"Software Forensics\": {{\n",
    "                \"Operating System Forensics\": {{\"File systems for Windows/Mac/Unix/Linux\": [], \"Windows/Mac/Unix/Linux\": []}},\n",
    "                \"Application Software Forensics\": {{\"Mail Services\": [], \"Web Services\": [],  \"DBMS\": [], \"Access Control Systems\": [\"Building security Logs\", \"Passport control Logs\"], \"E-Commerce Services\": [\"Credit Card Logs\", \"Bank Logs\", \"E-payment Logs\", \"WebShop Logs\"]}},\n",
    "                \"Forensic Tools Analysis (Open source/Proprietary)\": {{\"E4Case/FTK/File Hound/Sleuthkit/WinHex\": []}}\n",
    "            }},\n",
    "                \"Database Forensics\": {{\n",
    "                \"Database Metadata/Contents Forensics\": {{\"DBMS\": [], \"Databases\": []}}\n",
    "            }},\n",
    "                \"Multimedia Forensics\": {{\n",
    "                \"Image Forensics\": {{\"Digital Images\": []}},\n",
    "                \"Video Forensics\": {{\"Digital Video\": []}},\n",
    "                \"Audio Forensics\": {{\"Digital Audio\": []}}\n",
    "            }},\n",
    "                \"Device Forensics\": {{\n",
    "                \"Peripherals Device Forensics\": {{\"Copiers\": [], \"Printers\": [], \"Scanners\": []}}\n",
    "                \"Network Enabled Device Forensics\": {{\"Wireless AP\": [], \"IDS\": [], \"Firewalls\": [], \"Hubs\": [], \"Switches\": [], \"Routers\": []}},\n",
    "                \"Storage Device Forensics\": {{\"RFID Tags/Smart cards/Memory cards\": [], \"DVD/CD/Floppy/Tapes\": [], \"External Hard Drives\": [], \"Thumb Drive\": [], \"Digital Music Players\": [\"iPod\"]}},\n",
    "                \"Large Scale Device Forensics\": {{\"SAN (Storage Area Network)\": [], \"NAS (Network Attached Storage)\": []}},\n",
    "                \"Obscure Device Forensics\": {{\"Recording Devices (Video/Audio)\": [\"Camcorder\", \"surveillance camera\"], \"Gaming Devices\": [\"Play Station\", \"Xbox\", \"PSP\"]}},\n",
    "                \"Mobile Forensics\": {{\"PDAs\": [\"BlackBerry\"], \"Smart/Cell phones\": [\"Phone Memory Cards\", \"SIM Cards\"], \"Tablets\": []}}\n",
    "                \"Small Scale Device Forensics\": {{ \"Embedded Devices\": [], \"GPS Devices\": []}}\n",
    "            }},\n",
    "                \"Network Forensics\": {{\n",
    "                \"Cloud Forensics\": {{\"Clouds (Cloud Computing)\": []}},\n",
    "                \"Telecom Network Forensics\": {{\"Cell Phone/Telecom Service Provider Network\": []}},\n",
    "                \"Internet Forensics\": {{\"Web Documents\": [], \"Webmails\": [], \"Emails\": [], \"Domain Name Records\": [], \"ISP Logs\": []}},\n",
    "                \"Wireless Forensics\": {{\"Bluetooth, Infrared, Wi-Fi\": []}}\n",
    "            }},\n",
    "                \"IoT Forensics\": {{\n",
    "                \"Smart Home Devices\": {{\"IoT Cameras\": [], \"Thermostats\": [], \"Relays / Switch Actuators\":[]}},\n",
    "                \"Industrial IoT Systems\": {{\"SCADA\": [], \"ICS Logs\": []}},\n",
    "                \"Drone Forensic\":{{\"Drone (UAVs)\": [\" Flight Logs\", \"GPS Data\", \"Video Footage\", \"Sensor Data\", \"Communication Logs\"]}},\n",
    "                \"Embedded IoT Devices\": {{\n",
    "                  \"Operating Systems\": [\"Contiki / Contiki-NG\", \"RIOT\", \"TinyOS\", \"FreeRTOS\"],\n",
    "                  \"File Systems\": [\"Coffee\", \"YAFFS2\", \"LittleFS\", \"JFFS2\", \"FAT\"],\n",
    "                  \"Memory Artifacts\": [\"Flash Dumps\", \"Micrologs\", \"Wear-Leveling Metadata\"],\n",
    "                  \"Device Types\": [\"Motes (Sky, Z1)\", \"Sensor Nodes\", \"MCU Boards (CC13xx/26xx, CC2538)\"]\n",
    "                }}\n",
    "            }},\n",
    "                \"AI Forensics\": {{\n",
    "                \"AI Training Forensics\": {{\"Training Process Forensics\": [\"Training Chain of Custody\", \"Objective / Cost Function Forensics\", \"Pre-processing Forensics\"], \"Dataset Forensics\": [\"Collection Chain of Custody\", \"Labeling Chain of Custody\", \"QA Forensics\"],\"Environment Forensics\": []}},\n",
    "                \"AI Substrate Forensics\": {{\"Disk, Network, Sensor, Actuator\": [] }},\n",
    "                \"AI Application Forensics\": {{\"API, Artifacts\": []}},\n",
    "                \"AI Model Forensics\": {{\"Model Authentication Forensics, Model Fingerprinting / Ballistics Forensics, Model Identification / Extraction Forensics, Model Performance Forensics, Model Malware Forensics, Model Chain of Custody\": []}}\n",
    "            }},\n",
    "                \"Blockchain Forensics\": {{\n",
    "                \"Wallet Forensics\": {{\"Cryptocurrency Wallets\": [\"Public/Private Keys\"]}},\n",
    "                \"Transaction Analysis\": {{\"Blockchain Ledger\": [\"Smart Contracts\", \"Hashes\"]}}\n",
    "            }},\n",
    "               \"Knowledge Systematization\": {{\n",
    "                \"Systematization of Knowledge (SoK)\": [],\n",
    "                \"Systematic Literature Review (SLR)\": {{\"SLR Papers\": [], \"Comparative Studies\": []}},\n",
    "                \"Ontology Development/Taxonomy Development\": {{\"Ontologies\": [], \"Taxonomies\": []}},\n",
    "                \"Survey Papers\": [],\n",
    "                \"Frameworks\": {{\"Validation Methodologies\": [], \"Community Standards\": []}},\n",
    "                \"Methodological Awareness / Risk Analysis\": []\n",
    "            }}\n",
    "        }}\n",
    "\n",
    "            Your response must be in the following JSON format:\n",
    "            {{\n",
    "                \"discipline\": \"...\",\n",
    "                \"subdiscipline\": \"...\",\n",
    "                \"object\": \"...\",\n",
    "                \"subobject\": \"...\"\n",
    "            }}\n",
    "            \n",
    "            \\t5- Always assign the discipline and subdiscipline based on where the potential digital evidence is primarily collected \n",
    "            and how it is logically stored or accessed.\n",
    "            \\t-If the evidence is collected through physical acquisition (e.g., raw disk sectors, hardware memory dumps, chip-off), classify it under\n",
    "            **Computer Forensics** with the appropriate subdiscipline **Laptop/Desktop/Server** and objects (Disk, RAM, Registers, etc.).\n",
    "            \\t-If the evidence is collected through an operating system, software, or protocol layer (e.g., file system metadata, OS logs, \n",
    "            applications, network flows, cloud APIs), classify it under **Software Forensics**or **Network Forensics**, depending on the context.\n",
    "\n",
    "            \\t6- Examples:\n",
    "\n",
    "            \\t- In the paper *“Avoiding Burnout at the Digital Forensics Coalface”*, the authors perform a thematic synthesis \n",
    "            and propose evidence-based frameworks for stress management in the digital forensics workforce.\n",
    "\n",
    "            {{\n",
    "                \"discipline\": \"Knowledge Systematization\",\n",
    "                \"subdiscipline\": \"Frameworks\",\n",
    "                \"object\": \"Validation Methodologies\",\n",
    "                \"subobject\": \"\"\n",
    "            }}\n",
    "            \n",
    "            \\t- Another papers like the one that only propose standardized validation frameworks, test guidelines, or \n",
    "            community standards (e.g., Testing Disk Imaging Tools return **discipline** as **Knowledge Systematization**\n",
    "            under subdiscipline **Frameworks**.\n",
    "\n",
    "            \\t- There are multiple papers in our dataset that falls under **AI Forensics** please handle with care. \n",
    "            For AI forensics, classify each paper into one of the four AI Forensics subdisciplines based on its main focus. \n",
    "            Always base your decision on where the forensic data is acquired. If the data comes from training logs, datasets, or training environments, \n",
    "            classify it under **AI Training Forensics**. If it involves analyzing the AI model itself for tampering, theft, or authenticity, choose **AI Model Forensics**.\n",
    "            If the data is extracted from the hardware, embedded systems, memory, or sensors, assign it to **AI Substrate Forensics**. \n",
    "            If the focus is on logs, APIs, outputs, or user-facing artifacts generated during AI use, \n",
    "            then it belongs to **AI Application Forensics**.\n",
    "\n",
    "            The paper **ChatGPT for digital forensic investigation: The good, the bad, and the unknown** clearly fits into the AI Forensics ontology, here's the proper classification:\n",
    "            \n",
    "            {{\n",
    "                \"discipline\": \"AI Forensics\",\n",
    "                \"subdiscipline\": \"AI Application Forensics\",\n",
    "                \"object\": \"API, Artifacts\",\n",
    "                \"subobject\": \"\"\n",
    "\n",
    "            }}\n",
    "\n",
    "\n",
    "\n",
    "            Here is the paper content:\n",
    "            <Start of Paper Content>\n",
    "            {content}\n",
    "            <End of Paper Content>\n",
    "\n",
    "            Your response: \"\"\"\n",
    "            '''\n",
    "   \n",
    "    else:\n",
    "        raise ValueError(\"Invalid task\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b537d0-cc09-4938-ae7f-5b40f410c613",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env\n",
    "load_dotenv(\"api_key.env\")\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Your prompt generator import or definition:\n",
    "# Make sure generate_all_metadata_prompt is defined exactly as you have it\n",
    "\n",
    "# Example: \n",
    "# from your_prompt_file import generate_all_metadata_prompt\n",
    "\n",
    "# Main processor\n",
    "def process_papers_for_tasks(papers, tasks):\n",
    "    task_results = {}\n",
    "\n",
    "    for i, paper in enumerate(papers):\n",
    "        paper_title = paper['title']\n",
    "        print(f\"Processing paper: {paper_title}\")\n",
    "\n",
    "        task_results[paper_title] = {}\n",
    "\n",
    "        for task in tasks:\n",
    "            user_prompt = generate_all_metadata_prompt(task, paper)\n",
    "\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model='gpt-4o-mini',\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": user_prompt,\n",
    "                        }\n",
    "                    ],\n",
    "                    temperature=0.2,\n",
    "                    max_tokens=1500\n",
    "                )\n",
    "\n",
    "                response_text = response.choices[0].message.content\n",
    "                print(response_text)\n",
    "                task_results[paper_title][task] = response_text\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {task} for paper {i+1}: {e}\")\n",
    "                task_results[paper_title][task] = \"error: \" + str(e)\n",
    "\n",
    "    return task_results\n",
    "\n",
    "#  Replace 'papers' with your dataset\n",
    "# For testing, slice first 5 papers\n",
    "test_papers = papers[:]\n",
    "\n",
    "#  Define your actual tasks\n",
    "tasks = [\"title\", \"authors\", \"school_names\", \"author_countries\", \"conference\", \"published_year\", \"forensic_vs_anti\", \"ontology_classification\"]\n",
    "\n",
    "# Process papers\n",
    "all_results = process_papers_for_tasks(test_papers, tasks)\n",
    "\n",
    "# Save to CSV exactly like you had it\n",
    "output_csv_path = 'results_combined_prompts_new.csv'\n",
    "utput_jsonl_path = 'results_combined_prompts_new.jsonl'\n",
    "with open(output_csv_path, 'w', newline='', encoding='utf-8') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    headers = ['Paper Title'] + tasks\n",
    "    csv_writer.writerow(headers)\n",
    "\n",
    "    for paper_title, results in all_results.items():\n",
    "        row = [paper_title]\n",
    "        for task in tasks:\n",
    "            row.append(results.get(task, \"No result\"))\n",
    "        csv_writer.writerow(row)\n",
    "# Save to JSONL\n",
    "with open(output_jsonl, \"w\", encoding=\"utf-8\") as jsonl_file:\n",
    "    for title, results in processed_results.items():\n",
    "        jsonl_file.write(json.dumps({title: results}) + \"\\n\")\n",
    "\n",
    "print(f\"Final results saved to {output_csv} and {output_jsonl}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb3bb5c-b3e5-4eed-b49b-cf05ff63e0d8",
   "metadata": {},
   "source": [
    "DATA VISUALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66123ce-0edf-48f4-9f7f-70a333a99c9d",
   "metadata": {},
   "source": [
    "Venue × Discipline table (USA, EU, APAC) — counts + MEDIAN + CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250863b9-b192-45b2-8cd3-0bd38f3ef719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 8) Venue × Discipline table (USA, EU, APAC)\n",
    "# -------------------------------\n",
    "\n",
    "# Helper: map conference string -> venue bucket\n",
    "def map_conference_to_venue(conf_str: str) -> str:\n",
    "    s = (conf_str or \"\").strip().upper()\n",
    "    if \"USA\" in s:\n",
    "        return \"DFRWS USA\"\n",
    "    if \"EU\" in s or \"EUROPE\" in s:\n",
    "        return \"DFRWS EU\"\n",
    "    if \"APAC\" in s or \"ASIA\" in s or \"ASIA-PACIFIC\" in s or \"ASIA PACIFIC\" in s:\n",
    "        return \"DFRWS APAC\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "records = []\n",
    "for _, row in df.iterrows():\n",
    "    # parse fields\n",
    "    conf_json = safe_parse_json(row.get(\"conference\", \"\"))\n",
    "    ont_json  = safe_parse_json(row.get(\"ontology_classification\", \"\"))\n",
    "\n",
    "    conf_raw = conf_json.get(\"conference\", \"\")\n",
    "    venue    = map_conference_to_venue(conf_raw)\n",
    "\n",
    "    discipline = (ont_json.get(\"discipline\", \"\") or \"\").strip()\n",
    "\n",
    "  \n",
    "    # skip empties & unknown venues\n",
    "    if not discipline:\n",
    "        continue\n",
    "    if venue == \"Unknown\":\n",
    "        continue\n",
    "\n",
    "    records.append({\"venue\": venue, \"discipline\": discipline})\n",
    "\n",
    "venue_disc_df = pd.DataFrame(records)\n",
    "\n",
    "# Build full matrix, include all venues that have papers and all disciplines seen in your run\n",
    "venue_disc_matrix = (\n",
    "    pd.crosstab(venue_disc_df[\"venue\"], venue_disc_df[\"discipline\"])\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Order columns (disciplines) by global total descending\n",
    "col_order = venue_disc_matrix.sum(axis=0).sort_values(ascending=False).index\n",
    "venue_disc_matrix = venue_disc_matrix[col_order]\n",
    "\n",
    "# Ensure the row order is USA, EU, APAC (only those that exist)\n",
    "desired_rows = [v for v in [\"DFRWS USA\", \"DFRWS EU\", \"DFRWS APAC\"] if v in venue_disc_matrix.index]\n",
    "venue_disc_matrix = venue_disc_matrix.reindex(desired_rows)\n",
    "\n",
    "# Add summary columns\n",
    "venue_disc_matrix[\"TOTAL\"]   = venue_disc_matrix.sum(axis=1)\n",
    "venue_disc_matrix[\"AVERAGE\"] = (venue_disc_matrix[col_order].mean(axis=1)).round(2)\n",
    "\n",
    "print(\"\\n=== Venue × Discipline (counts) ===\")\n",
    "print(venue_disc_matrix)\n",
    "\n",
    "# Save for your paper/supplement\n",
    "venue_disc_matrix.to_csv(\"venue_discipline_matrix.csv\")\n",
    "print(\"\\nSaved: venue_discipline_matrix.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5c7f8-af05-4ea7-ab21-36f25db6ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "disc_cols = [c for c in venue_disc_matrix.columns if c not in [\"TOTAL\",\"AVERAGE\"]]\n",
    "\n",
    "# Average across all disciplines (zeros counted) – what you already have\n",
    "venue_disc_matrix[\"AVERAGE_ALL\"] = (venue_disc_matrix[disc_cols].mean(axis=1)).round(2)\n",
    "\n",
    "# Average across non-zero disciplines only\n",
    "venue_disc_matrix[\"AVERAGE_NONZERO\"] = (\n",
    "    venue_disc_matrix[disc_cols].replace(0, pd.NA).mean(axis=1)\n",
    ").round(2)\n",
    "\n",
    "# Median per venue (robust to outliers)\n",
    "venue_disc_matrix[\"MEDIAN\"] = venue_disc_matrix[disc_cols].median(axis=1).round(2)\n",
    "\n",
    "# Coefficient of variation (std/mean) – lower = more consistent across domains\n",
    "means = venue_disc_matrix[disc_cols].mean(axis=1).replace(0, np.nan)\n",
    "stds  = venue_disc_matrix[disc_cols].std(axis=1, ddof=0)\n",
    "venue_disc_matrix[\"CV\"] = (stds / means).round(3)\n",
    "\n",
    "# (Optional) Keep just one \"AVERAGE\" and rename it:\n",
    "venue_disc_matrix[\"AVERAGE\"] = venue_disc_matrix[\"AVERAGE_ALL\"].round(2)\n",
    "venue_disc_matrix.drop(columns=[\"AVERAGE_ALL\"], inplace=True)\n",
    "\n",
    "# Show full table and save\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None,\n",
    "                       \"display.width\", 200, \"display.max_colwidth\", None):\n",
    "    print(\"\\n=== Venue × Discipline (counts + averages) ===\")\n",
    "    print(venue_disc_matrix.to_string())\n",
    "\n",
    "venue_disc_matrix.to_csv(\"venue_discipline_matrix_with_stats.csv\")\n",
    "print(\"\\nSaved: venue_discipline_matrix_with_stats.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0aa8b-b3cb-4d0e-bddd-0c0da3b3c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 8) Venue × Discipline table (USA, EU, APAC) — robust + MEDIAN & CV\n",
    "# -------------------------------\n",
    "\n",
    "def map_conference_to_venue(conf_str: str) -> str:\n",
    "    s = (conf_str or \"\").strip().upper()\n",
    "    if \"USA\" in s:\n",
    "        return \"DFRWS USA\"\n",
    "    if \"EU\" in s or \"EUROPE\" in s:\n",
    "        return \"DFRWS EU\"\n",
    "    if \"APAC\" in s or \"ASIA\" in s or \"ASIA-PACIFIC\" in s or \"ASIA PACIFIC\" in s:\n",
    "        return \"DFRWS APAC\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "def extract_conference(row) -> str:\n",
    "    \"\"\"\n",
    "    Try JSON in 'conference'; if that fails, try plain strings / alternate cols.\n",
    "    \"\"\"\n",
    "    # 1) JSON cell like {\"conference\":\"DFRWS USA\"}\n",
    "    conf_json = safe_parse_json(row.get(\"conference\", \"\"))\n",
    "    if isinstance(conf_json, dict):\n",
    "        val = conf_json.get(\"conference\", \"\") or conf_json.get(\"name\", \"\")\n",
    "        if isinstance(val, str) and val.strip():\n",
    "            return val.strip()\n",
    "\n",
    "    # 2) If the cell itself is already a plain string like \"DFRWS USA\"\n",
    "    raw = row.get(\"conference\", \"\")\n",
    "    if isinstance(raw, str) and raw.strip():\n",
    "        return raw.strip()\n",
    "\n",
    "    # 3) Fallback column names you might have\n",
    "    for alt in (\"venue\", \"conf\", \"event\", \"conference_name\"):\n",
    "        if alt in row and isinstance(row[alt], str) and row[alt].strip():\n",
    "            return row[alt].strip()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def extract_discipline(row) -> str:\n",
    "    \"\"\"\n",
    "    Prefer a flat 'discipline' column if present; else parse ontology JSON.\n",
    "    \"\"\"\n",
    "    if \"discipline\" in df.columns:\n",
    "        val = row.get(\"discipline\", \"\")\n",
    "        if isinstance(val, str) and val.strip():\n",
    "            return val.strip()\n",
    "\n",
    "    ont_json = safe_parse_json(row.get(\"ontology_classification\", \"\"))\n",
    "    if isinstance(ont_json, dict):\n",
    "        val = (ont_json.get(\"discipline\") or \"\")\n",
    "        if isinstance(val, str) and val.strip():\n",
    "            return val.strip()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "records = []\n",
    "debug_counts = {\"rows\": 0, \"kept\": 0, \"no_conf\": 0, \"unknown_venue\": 0, \"no_disc\": 0}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    debug_counts[\"rows\"] += 1\n",
    "\n",
    "    conf_raw = extract_conference(row)\n",
    "    venue = map_conference_to_venue(conf_raw)\n",
    "    disc = extract_discipline(row)\n",
    "\n",
    "\n",
    "    if not disc:\n",
    "        debug_counts[\"no_disc\"] += 1\n",
    "        continue\n",
    "    if not conf_raw:\n",
    "        debug_counts[\"no_conf\"] += 1\n",
    "        continue\n",
    "    if venue == \"Unknown\":\n",
    "        debug_counts[\"unknown_venue\"] += 1\n",
    "        continue\n",
    "\n",
    "    records.append({\"venue\": venue, \"discipline\": disc})\n",
    "    debug_counts[\"kept\"] += 1\n",
    "\n",
    "venue_disc_df = pd.DataFrame.from_records(records, columns=[\"venue\", \"discipline\"])\n",
    "\n",
    "# If still empty, fail gracefully with actionable info\n",
    "if venue_disc_df.empty:\n",
    "    print(\"\\n[WARN] No venue/discipline records parsed. Debug:\", debug_counts)\n",
    "    print(\"Tip: Inspect a few raw cells, e.g.:\")\n",
    "    # show a few examples from your dataframe to help verify formats\n",
    "    print(\" - df['conference'].head() =\", df.get(\"conference\", pd.Series(dtype=object)).head().to_list())\n",
    "    print(\" - df['ontology_classification'].head() =\", df.get(\"ontology_classification\", pd.Series(dtype=object)).head().to_list())\n",
    "else:\n",
    "    # Build matrix\n",
    "    venue_disc_matrix = pd.crosstab(venue_disc_df[\"venue\"], venue_disc_df[\"discipline\"]).astype(int)\n",
    "\n",
    "    # Order columns by global total (desc)\n",
    "    col_order = venue_disc_matrix.sum(axis=0).sort_values(ascending=False).index\n",
    "    venue_disc_matrix = venue_disc_matrix[col_order]\n",
    "\n",
    "    # Row order: USA, EU, APAC (only those present)\n",
    "    desired_rows = [v for v in [\"DFRWS USA\", \"DFRWS EU\", \"DFRWS APAC\"] if v in venue_disc_matrix.index]\n",
    "    venue_disc_matrix = venue_disc_matrix.reindex(desired_rows)\n",
    "\n",
    "    # ---- Stats (no \"non-zero\" variants) ----\n",
    "    core = venue_disc_matrix[col_order]\n",
    "\n",
    "    venue_disc_matrix[\"TOTAL\"]  = core.sum(axis=1)\n",
    "\n",
    "    # Median across disciplines (including zeros, per your last preference)\n",
    "    venue_disc_matrix[\"MEDIAN\"] = core.median(axis=1).round(2)\n",
    "\n",
    "    # Coefficient of Variation = std/mean (protect against division by zero)\n",
    "    means = core.mean(axis=1)\n",
    "    stds  = core.std(axis=1, ddof=1)  # sample std\n",
    "    cv = (stds / means.replace(0, pd.NA)).fillna(0.0).round(3)\n",
    "    venue_disc_matrix[\"CV\"] = cv\n",
    "\n",
    "    print(\"\\n=== Venue × Discipline (counts + MEDIAN & CV) ===\")\n",
    "    print(venue_disc_matrix)\n",
    "\n",
    "    venue_disc_matrix.to_csv(\"venue_discipline_matrix.csv\")\n",
    "    print(\"\\nSaved: venue_discipline_matrix.csv\")\n",
    "    print(\"\\n[Debug summary]\", debug_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e1f71-b757-43f3-bddb-ab4761744e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load CSV File\n",
    "# -------------------------------\n",
    "csv_path = \"results_combined_prompts_new.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Safe JSON Parser\n",
    "# -------------------------------\n",
    "def safe_parse_json(cell):\n",
    "    try:\n",
    "        cell = re.sub(r\"```json|```\", \"\", str(cell)).strip()\n",
    "        return json.loads(cell)\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Extract and Normalize Fields\n",
    "# -------------------------------\n",
    "discipline_year_counts = defaultdict(lambda: defaultdict(int))\n",
    "subdiscipline_year_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    ont = safe_parse_json(row.get(\"ontology_classification\", \"\"))\n",
    "    year_entry = safe_parse_json(row.get(\"published_year\", \"\"))\n",
    "    year = year_entry.get(\"year\", None)\n",
    "\n",
    "    if year:\n",
    "        try:\n",
    "            year = int(year)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        discipline = ont.get(\"discipline\", \"\").strip()\n",
    "        subdiscipline = ont.get(\"subdiscipline\", \"\").strip()\n",
    "\n",
    "        # Normalize: Merge Mobile Forensics into Device Forensics\n",
    "       \n",
    "        # NEW normalization requested:\n",
    "        if subdiscipline == \"Smart/Cell phones\":           #because its an object \n",
    "            subdiscipline = \"Small Scale Device Forensics\"\n",
    "            \n",
    "        if discipline:\n",
    "            discipline_year_counts[discipline][year] += 1\n",
    "        if subdiscipline:\n",
    "            subdiscipline_year_counts[subdiscipline][year] += 1\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Convert to DataFrames\n",
    "# -------------------------------\n",
    "discipline_df = pd.DataFrame(discipline_year_counts).fillna(0).astype(int).sort_index()\n",
    "subdiscipline_df = pd.DataFrame(subdiscipline_year_counts).fillna(0).astype(int).sort_index()\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Plot Line Graph: Discipline Trends\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(14, 6))\n",
    "discipline_df.rolling(window=2, min_periods=1).mean().plot()\n",
    "plt.title(\"Discipline Trends in DFRWS (2002–2025)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Papers\")\n",
    "plt.legend(title=\"Discipline\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Plot Line Graph: Subdiscipline Trends\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(14, 6))\n",
    "subdiscipline_df.rolling(window=2, min_periods=1).mean().plot()\n",
    "plt.title(\"Subdiscipline Trends in DFRWS (2002–2025)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Papers\")\n",
    "plt.legend(title=\"Subdiscipline\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Print and Save Totals\n",
    "# -------------------------------\n",
    "discipline_totals = discipline_df.sum().sort_values(ascending=False)\n",
    "subdiscipline_totals = subdiscipline_df.sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop Disciplines by Total Count:\")\n",
    "print(discipline_totals)\n",
    "\n",
    "print(\"\\nTop Subdisciplines by Total Count:\")\n",
    "print(subdiscipline_totals)\n",
    "\n",
    "# Save total counts\n",
    "discipline_totals.to_csv(\"discipline_totals.csv\")\n",
    "subdiscipline_totals.to_csv(\"subdiscipline_totals.csv\")\n",
    "\n",
    "#  Save yearly trends\n",
    "discipline_df.to_csv(\"discipline_yearly_trends.csv\")\n",
    "subdiscipline_df.to_csv(\"subdiscipline_yearly_trends.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0a7e16-7c40-43ca-9b1a-165d214dac78",
   "metadata": {},
   "source": [
    "TOP 15 SUBDOMAINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a77fe-945f-4192-a2ed-80fcc746532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load CSV File\n",
    "# -------------------------------\n",
    "csv_path = \"results_combined_prompts_new.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Safe JSON Parser\n",
    "# -------------------------------\n",
    "def safe_parse_json(cell):\n",
    "    try:\n",
    "        cell = re.sub(r\"```json|```\", \"\", str(cell)).strip()\n",
    "        return json.loads(cell)\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Extract and Normalize Fields\n",
    "# -------------------------------\n",
    "discipline_year_counts = defaultdict(lambda: defaultdict(int))\n",
    "subdiscipline_year_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    ont = safe_parse_json(row.get(\"ontology_classification\", \"\"))\n",
    "    year_entry = safe_parse_json(row.get(\"published_year\", \"\"))\n",
    "    year = year_entry.get(\"year\", None)\n",
    "\n",
    "    if year:\n",
    "        try:\n",
    "            year = int(year)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        discipline = ont.get(\"discipline\", \"\").strip()\n",
    "        subdiscipline = ont.get(\"subdiscipline\", \"\").strip()\n",
    "\n",
    "        if subdiscipline == \"Smart/Cell phones\":\n",
    "            subdiscipline = \"Small Scale Device Forensics\"\n",
    "\n",
    "        if discipline:\n",
    "            discipline_year_counts[discipline][year] += 1\n",
    "        if subdiscipline:\n",
    "            subdiscipline_year_counts[subdiscipline][year] += 1\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Convert to DataFrames\n",
    "# -------------------------------\n",
    "discipline_df = pd.DataFrame(discipline_year_counts).fillna(0).astype(int).sort_index()\n",
    "subdiscipline_df = pd.DataFrame(subdiscipline_year_counts).fillna(0).astype(int).sort_index()\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Plot Line Graph: Discipline Trends (unchanged)\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(14, 6))\n",
    "discipline_df.rolling(window=2, min_periods=1).mean().plot()\n",
    "plt.title(\"Discipline Trends in DFRWS (2002–2025)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Papers\")\n",
    "plt.legend(title=\"Discipline\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Plot Line Graph: Subdiscipline Trends (unchanged)\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(14, 6))\n",
    "subdiscipline_df.rolling(window=2, min_periods=1).mean().plot()\n",
    "plt.title(\"Subdiscipline Trends in DFRWS (2002–2025)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Papers\")\n",
    "plt.legend(title=\"Subdiscipline\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Save ONLY one CSV: Yearly trend for Top 15 subdisciplines\n",
    "# -------------------------------\n",
    "subdiscipline_totals = subdiscipline_df.sum().sort_values(ascending=False)\n",
    "top15_subs = subdiscipline_totals.head(15).index.tolist()\n",
    "\n",
    "sub_top15 = subdiscipline_df[top15_subs].copy()\n",
    "sub_top15.index.name = \"year\"\n",
    "sub_top15.to_csv(\"subdiscipline_yearly_trends_top15.csv\")\n",
    "print(\"Saved: subdiscipline_yearly_trends_top15.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28d8747-37af-4479-9142-61dd8f9174df",
   "metadata": {},
   "source": [
    "Domain Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8839c0-2517-44e5-ada0-6494be19185b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# forensics_trends.py\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# --------------------- Config ---------------------\n",
    "CSV_PATH = Path(\"results_combined_prompts_new.csv\")  # input\n",
    "PLOT_PATH = Path(\"discipline_trends_all.png\")\n",
    "\n",
    "CSV_DISC_TOTALS = Path(\"all_disciplines_2002_2025.csv\")\n",
    "CSV_SUB_TOTALS = Path(\"all_subdisciplines_2002_2025.csv\")\n",
    "CSV_DISC_MATRIX = Path(\"discipline_by_year_with_totals.csv\")\n",
    "CSV_SUB_MATRIX = Path(\"subdiscipline_by_year_with_totals.csv\")\n",
    "\n",
    "\n",
    "# ----------------- Parsing Helpers ----------------\n",
    "def strip_code_fences(s: str) -> str:\n",
    "    \"\"\"Remove ```json ... ``` or bare ``` fences (and extra whitespace).\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s).strip()\n",
    "    # remove starting ```json or ```\n",
    "    s = re.sub(r\"^\\s*```json\\s*\", \"\", s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r\"^\\s*```\\s*\", \"\", s)\n",
    "    # remove trailing ```\n",
    "    s = re.sub(r\"\\s*```\\s*$\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def parse_year(cell: str):\n",
    "    \"\"\"Return int year or None.\"\"\"\n",
    "    try:\n",
    "        j = json.loads(strip_code_fences(cell))\n",
    "        y = j.get(\"year\")\n",
    "        if y in (None, \"\", \"Unknown\"):\n",
    "            return None\n",
    "        return int(str(y))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_ontology(cell: str):\n",
    "    \"\"\"Return (discipline, subdiscipline). If missing, return empty strings.\"\"\"\n",
    "    try:\n",
    "        j = json.loads(strip_code_fences(cell))\n",
    "        return j.get(\"discipline\", \"\") or \"\", j.get(\"subdiscipline\", \"\") or \"\"\n",
    "    except Exception:\n",
    "        return \"\", \"\"\n",
    "\n",
    "\n",
    "# -------------------- Load & Clean -----------------\n",
    "def load_clean_df(csv_path: Path) -> pd.DataFrame:\n",
    "    raw = pd.read_csv(csv_path, dtype=str, keep_default_na=False)\n",
    "    # normalize header names\n",
    "    raw.columns = raw.columns.str.strip().str.lower()\n",
    "\n",
    "    # parse fields safely\n",
    "    year = raw.get(\"published_year\", \"\").apply(parse_year)\n",
    "    disc_sub = raw.get(\"ontology_classification\", \"\").apply(parse_ontology)\n",
    "    discipline = disc_sub.apply(lambda t: t[0]).astype(str).str.strip()\n",
    "    subdiscipline = disc_sub.apply(lambda t: t[1]).astype(str).str.strip()\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"year\": year,\n",
    "            \"discipline\": discipline.replace({\"None\": \"\", \"null\": \"\"}),\n",
    "            \"subdiscipline\": subdiscipline.replace({\"None\": \"\", \"null\": \"\"}),\n",
    "        }\n",
    "    )\n",
    "    # unify blanks as \"Unknown\"\n",
    "    df[\"discipline\"] = df[\"discipline\"].replace(\"\", \"Unknown\")\n",
    "    df[\"subdiscipline\"] = df[\"subdiscipline\"].replace(\"\", \"Unknown\")\n",
    "    # drop rows without year and cast to int\n",
    "    df = df.dropna(subset=[\"year\"]).astype({\"year\": int})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------- ALL disciplines/subdisciplines with totals --------\n",
    "def by_year_counts(df: pd.DataFrame, column: str):\n",
    "    out = {}\n",
    "    for yr, grp in df.groupby(\"year\", sort=True):\n",
    "        counts = grp[column].value_counts(dropna=False).sort_index()\n",
    "        total = int(counts.sum())\n",
    "        out[yr] = {\n",
    "            \"rows\": [(name, int(cnt), (cnt / total) * 100) for name, cnt in counts.items()],\n",
    "            \"total\": total,\n",
    "        }\n",
    "    return out\n",
    "\n",
    "\n",
    "def print_section_with_totals(title: str, year_map: dict):\n",
    "    print(f\"\\n{title}\")\n",
    "    grand = 0\n",
    "    for yr in sorted(year_map):\n",
    "        print(f\"\\nYear: {yr}\")\n",
    "        for name, cnt, pct in year_map[yr][\"rows\"]:\n",
    "            print(f\"  {name}: {cnt} ({pct:.2f}%)\")\n",
    "        print(f\"  TOTAL: {year_map[yr]['total']}\")\n",
    "        grand += year_map[yr][\"total\"]\n",
    "    return grand\n",
    "\n",
    "\n",
    "# -------------------- Plotting ---------------------\n",
    "def plot_all_disciplines(df: pd.DataFrame, out_path: Path):\n",
    "    \"\"\"\n",
    "    Line chart of ALL disciplines by year.\n",
    "    (If there are many lines, the legend is placed outside to reduce clutter.)\n",
    "    \"\"\"\n",
    "    # matrix: rows=year, cols=discipline; zeros filled\n",
    "    mat = pd.crosstab(df[\"year\"], df[\"discipline\"]).sort_index()\n",
    "\n",
    "    # make the plot\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    for col in mat.columns:\n",
    "        plt.plot(mat.index, mat[col], marker=\"o\", linewidth=1.8, label=col)\n",
    "\n",
    "    plt.title(\"Discipline Trends by Year (All Disciplines)\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    # legend outside right; may be long, but you asked for all\n",
    "    plt.legend(title=\"Discipline\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ------------------------- Main -------------------------\n",
    "def main():\n",
    "    if not CSV_PATH.exists():\n",
    "        print(f\"ERROR: '{CSV_PATH}' not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    df = load_clean_df(CSV_PATH)\n",
    "    print(f\"Rows loaded (non-empty year): {len(df)}\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "    # Per-year maps\n",
    "    disc_year = by_year_counts(df, \"discipline\")\n",
    "    sub_year = by_year_counts(df, \"subdiscipline\")\n",
    "\n",
    "    # Print sections with totals\n",
    "    gdisc = print_section_with_totals(\"Discipline trends (ALL values):\", disc_year)\n",
    "    gsub = print_section_with_totals(\"Subdiscipline trends (ALL values):\", sub_year)\n",
    "\n",
    "    # Final overall totals\n",
    "    disc_totals = df[\"discipline\"].value_counts().reset_index()\n",
    "    disc_totals.columns = [\"Discipline\", \"Count\"]\n",
    "    sub_totals = df[\"subdiscipline\"].value_counts().reset_index()\n",
    "    sub_totals.columns = [\"Subdiscipline\", \"Count\"]\n",
    "\n",
    "    print(\"\\n=== FINAL TOTALS ===\")\n",
    "    print(f\"Rows used: {len(df)}\")\n",
    "    print(f\"Sum of per-year discipline totals:   {gdisc}\")\n",
    "    print(f\"Sum of per-year subdiscipline totals: {gsub}\")\n",
    "\n",
    "    print(\"\\nOverall discipline totals:\")\n",
    "    for _, r in disc_totals.iterrows():\n",
    "        print(f\"  {r['Discipline']}: {int(r['Count'])}\")\n",
    "    print(f\"  GRAND TOTAL: {int(disc_totals['Count'].sum())}\")\n",
    "\n",
    "    print(\"\\nOverall subdiscipline totals:\")\n",
    "    for _, r in sub_totals.iterrows():\n",
    "        print(f\"  {r['Subdiscipline']}: {int(r['Count'])}\")\n",
    "    print(f\"  GRAND TOTAL: {int(sub_totals['Count'].sum())}\")\n",
    "\n",
    "    # Wide matrices with row/column totals\n",
    "    disc_matrix = pd.crosstab(df[\"year\"], df[\"discipline\"]).sort_index()\n",
    "    disc_matrix[\"TOTAL\"] = disc_matrix.sum(axis=1)\n",
    "    disc_matrix.loc[\"GRAND_TOTAL\"] = disc_matrix.sum(axis=0)\n",
    "\n",
    "    sub_matrix = pd.crosstab(df[\"year\"], df[\"subdiscipline\"]).sort_index()\n",
    "    sub_matrix[\"TOTAL\"] = sub_matrix.sum(axis=1)\n",
    "    sub_matrix.loc[\"GRAND_TOTAL\"] = sub_matrix.sum(axis=0)\n",
    "\n",
    "    # Save CSVs\n",
    "    disc_totals.to_csv(CSV_DISC_TOTALS, index=False)\n",
    "    sub_totals.to_csv(CSV_SUB_TOTALS, index=False)\n",
    "    disc_matrix.to_csv(CSV_DISC_MATRIX)\n",
    "    sub_matrix.to_csv(CSV_SUB_MATRIX)\n",
    "\n",
    "    # Plot ALL disciplines over years\n",
    "    plot_all_disciplines(df, PLOT_PATH)\n",
    "\n",
    "    print(\"\\nSaved files:\")\n",
    "    print(f\"  {CSV_DISC_TOTALS}\")\n",
    "    print(f\"  {CSV_SUB_TOTALS}\")\n",
    "    print(f\"  {CSV_DISC_MATRIX}\")\n",
    "    print(f\"  {CSV_SUB_MATRIX}\")\n",
    "    print(f\"  {PLOT_PATH}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3363ba-e408-4699-b6b3-0219e812b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "INPUT_CSV = \"results_combined_prompts_new.csv\"\n",
    "YEAR_MIN, YEAR_MAX = 2002, 2025   # change if needed\n",
    "\n",
    "# ---------- helpers ----------\n",
    "FENCE_RE = re.compile(r\"^```json\\s*|\\s*```$\", re.IGNORECASE)\n",
    "\n",
    "def parse_fenced_json(cell, default=None):\n",
    "    \"\"\"\n",
    "    Accepts strings like:\n",
    "      \"```json\\n{ \\\"year\\\": \\\"2007\\\" }\\n```\"\n",
    "      or plain JSON, or already-dict.\n",
    "    Returns dict or `default`.\n",
    "    \"\"\"\n",
    "    if isinstance(cell, dict):\n",
    "        return cell\n",
    "    if not isinstance(cell, str) or not cell.strip():\n",
    "        return default\n",
    "    s = FENCE_RE.sub(\"\", cell.strip())  # drop ```json ... ```\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def coerce_year(y):\n",
    "    \"\"\"Return int year if valid, else None.\"\"\"\n",
    "    try:\n",
    "        y = int(str(y).strip())\n",
    "        return y if (YEAR_MIN <= y <= YEAR_MAX) else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------- load & parse ----------\n",
    "raw = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Parse published_year -> year (int)\n",
    "pub_parsed = raw[\"published_year\"].apply(parse_fenced_json, default={})\n",
    "years = pub_parsed.apply(lambda d: (d or {}).get(\"year\"))\n",
    "years = years.apply(coerce_year)\n",
    "\n",
    "# Parse ontology_classification -> discipline/subdiscipline\n",
    "onto_parsed = raw[\"ontology_classification\"].apply(parse_fenced_json, default={})\n",
    "discipline = onto_parsed.apply(lambda d: (d or {}).get(\"discipline\", \"\")).fillna(\"\")\n",
    "subdiscipline = onto_parsed.apply(lambda d: (d or {}).get(\"subdiscipline\", \"\")).fillna(\"\")\n",
    "\n",
    "# Build the cleaned DF\n",
    "df = pd.DataFrame({\n",
    "    \"year\": years,\n",
    "    \"discipline\": discipline.astype(str).str.strip(),\n",
    "    \"subdiscipline\": subdiscipline.astype(str).str.strip(),\n",
    "}).dropna(subset=[\"year\"])\n",
    "\n",
    "df[\"year\"] = df[\"year\"].astype(int)\n",
    "\n",
    "# ---------- discipline trend matrix (ALL disciplines, zeros filled) ----------\n",
    "counts = (\n",
    "    df.groupby([\"year\", \"discipline\"])\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "all_years = list(range(YEAR_MIN, YEAR_MAX + 1))\n",
    "mat = (\n",
    "    counts.pivot(index=\"year\", columns=\"discipline\", values=\"count\")\n",
    "          .reindex(all_years, fill_value=0)\n",
    ")\n",
    "\n",
    "# make sure matrix is all integer zeros (no NaN)\n",
    "mat = mat.fillna(0).astype(int)\n",
    "\n",
    "# ---------- export matrix used for the line chart (zeros in CSV) ----------\n",
    "mat.to_csv(\"discipline_trend_by_year_all.csv\", na_rep=\"0\")\n",
    "print(\"Saved: discipline_trend_by_year_all.csv\")\n",
    "\n",
    "# ---------- “with totals” wide table ----------\n",
    "disc_matrix = pd.crosstab(df[\"year\"], df[\"discipline\"]).sort_index()\n",
    "# guarantee year rows in range even if no papers that year\n",
    "disc_matrix = disc_matrix.reindex(index=all_years, fill_value=0)\n",
    "disc_matrix[\"TOTAL\"] = disc_matrix.sum(axis=1)\n",
    "disc_matrix.loc[\"GRAND_TOTAL\"] = disc_matrix.sum(axis=0)\n",
    "\n",
    "disc_matrix = disc_matrix.fillna(0).astype(int)\n",
    "disc_matrix.to_csv(\"discipline_by_year_with_totals.csv\", na_rep=\"0\")\n",
    "print(\"Saved: discipline_by_year_with_totals.csv\")\n",
    "\n",
    "# ---------- plot ALL discipline lines ----------\n",
    "# (Order columns so high-volume lines are drawn last and remain visible)\n",
    "order = mat.sum(axis=0).sort_values(ascending=True).index\n",
    "mat = mat[order]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "for col in mat.columns:\n",
    "    plt.plot(mat.index, mat[col], marker=\"o\", linewidth=1.8, alpha=0.9, label=col)\n",
    "\n",
    "plt.title(\"DFRWS Disciplines by Year (All Disciplines)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(all_years, rotation=45)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "plt.legend(\n",
    "    title=\"Discipline\",\n",
    "    fontsize=8,\n",
    "    title_fontsize=9,\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1.02, 0.5),\n",
    "    frameon=False,\n",
    "    ncol=1\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd84b458-929d-4496-a6e2-75de013e77bc",
   "metadata": {},
   "source": [
    "Key Domains , All papers and CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3e9066-2cf4-4ef5-a330-0eb7e8c68b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, json, re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- config ----------\n",
    "INPUT_CSV = \"results_combined_prompts_new.csv\"\n",
    "YEAR_MIN, YEAR_MAX = 2002, 2025\n",
    "all_years = list(range(YEAR_MIN, YEAR_MAX + 1))\n",
    "\n",
    "# ---------- robust parsers ----------\n",
    "def strip_fences(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    # remove ANY ``` or ```json occurrences, anywhere in the cell\n",
    "    s = re.sub(r\"```(?:json)?\", \"\", s, flags=re.IGNORECASE)\n",
    "    s = s.replace(\"```\", \"\")\n",
    "    return s.strip()\n",
    "\n",
    "def parse_json_messy(cell, default=None):\n",
    "    if isinstance(cell, dict):\n",
    "        return cell\n",
    "    s = strip_fences(cell)\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        # fallback: pull \"year\": \"2007\" with regex if JSON is broken\n",
    "        m = re.search(r'\"year\"\\s*:\\s*\"?(?P<y>\\d{4})\"?', s or \"\")\n",
    "        if m:\n",
    "            return {\"year\": m.group(\"y\")}\n",
    "        return default if default is not None else {}\n",
    "\n",
    "def coerce_year(y):\n",
    "    try:\n",
    "        y = int(str(y).strip())\n",
    "        return y if YEAR_MIN <= y <= YEAR_MAX else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------- rebuild df (year + discipline) ----------\n",
    "raw = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "years = raw.get(\"published_year\", pd.Series([\"\"] * len(raw))).apply(parse_json_messy, default={})\n",
    "years = years.apply(lambda d: (d or {}).get(\"year\"))\n",
    "years = years.apply(coerce_year)\n",
    "\n",
    "onto = raw.get(\"ontology_classification\", pd.Series([\"\"] * len(raw))).apply(parse_json_messy, default={})\n",
    "discipline = onto.apply(lambda d: (d or {}).get(\"discipline\", \"\")).fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "df = pd.DataFrame({\"year\": years, \"discipline\": discipline}).dropna(subset=[\"year\"])\n",
    "df[\"year\"] = df[\"year\"].astype(int)\n",
    "\n",
    "# ---------- year × discipline matrix (zeros for missing years) ----------\n",
    "counts = (\n",
    "    df.groupby([\"year\", \"discipline\"])\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    ")\n",
    "mat = (\n",
    "    counts.pivot(index=\"year\", columns=\"discipline\", values=\"count\")\n",
    "          .reindex(all_years, fill_value=0)\n",
    "          .fillna(0).astype(int)\n",
    ")\n",
    "\n",
    "# ---------- CDF-style: ALL papers + Top-5 disciplines ----------\n",
    "# ALL papers per year\n",
    "yearly_all = df.groupby(\"year\").size().reindex(all_years, fill_value=0)\n",
    "\n",
    "# Top-5 disciplines by total volume\n",
    "top5 = mat.sum(axis=0).sort_values(ascending=False).head(6).index.tolist()\n",
    "print(\"Top-5 disciplines:\", top5)\n",
    "\n",
    "# Yearly counts table for ALL + Top-5\n",
    "cdf_counts = pd.DataFrame({\"ALL Papers\": yearly_all})\n",
    "for d in top5:\n",
    "    cdf_counts[d] = mat[d].reindex(all_years, fill_value=0)\n",
    "\n",
    "# Cumulative sums (CDF curves)\n",
    "cdf_cum = cdf_counts.cumsum()\n",
    "\n",
    "# Plot cumulative counts\n",
    "plt.figure(figsize=(14, 7))\n",
    "for col in cdf_cum.columns:\n",
    "    plt.plot(cdf_cum.index, cdf_cum[col], linewidth=2.2, label=col)\n",
    "plt.title(\"Cumulative Papers by Year (ALL + Top-6 Disciplines)\")\n",
    "plt.xlabel(\"Year\"); plt.ylabel(\"Cumulative Count\")\n",
    "plt.xticks(all_years, rotation=45); plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "plt.legend(title=\"Series\", loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# (Optional) Normalized CDFs (0..1) to compare timing of growth\n",
    "cdf_norm = cdf_cum.div(cdf_cum.iloc[-1])\n",
    "plt.figure(figsize=(14, 7))\n",
    "for col in cdf_norm.columns:\n",
    "    plt.plot(cdf_norm.index, cdf_norm[col], linewidth=2.0, label=col)\n",
    "plt.title(\"Normalized CDF (Proportion Reached by Year)\")\n",
    "plt.xlabel(\"Year\"); plt.ylabel(\"Proportion of Final Total\"); plt.ylim(0, 1.02)\n",
    "plt.xticks(all_years, rotation=45); plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "plt.legend(title=\"Series\", loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Exports\n",
    "cdf_counts.to_csv(\"cdf_all_plus_top6_yearly_counts.csv\")\n",
    "cdf_cum.to_csv(\"cdf_all_plus_top6_cumulative_counts.csv\")\n",
    "cdf_norm.to_csv(\"cdf_all_plus_top6_normalized_cdf.csv\")\n",
    "print(\"Saved CDF CSVs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34242d2f-085f-4fc0-9805-a876f89c6229",
   "metadata": {},
   "source": [
    "SCHOOL AFFILIATION(ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b8cec5-c7d7-4738-a0d2-ebfd9c5e9151",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "csv_path = \"results_combined_prompts_new.csv\"  # Ensure this file is in the same folder\n",
    "year_min, year_max = 2001, 2025               # Inclusive range\n",
    "\n",
    "# Helpers\n",
    "def safe_parse_json(cell):\n",
    "    \"\"\"\n",
    "    Parse cells that should contain JSON.\n",
    "    Strips code fences like ```json ... ``` if present.\n",
    "    Returns {} on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cleaned = re.sub(r\"```json|```\", \"\", str(cell)).strip()\n",
    "        return json.loads(cleaned)\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def is_year_in_range(entry, lo=year_min, hi=year_max):\n",
    "    try:\n",
    "        year = int(entry.get(\"year\", \"0\"))\n",
    "        return lo <= year <= hi\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def is_dfrws_any(entry):\n",
    "    \"\"\"\n",
    "    Match ALL DFRWS venues (USA, EU, APAC, etc.)\n",
    "    Expects parsed conference like {\"conference\": \"DFRWS USA\"}.\n",
    "    \"\"\"\n",
    "    conf = str(entry.get(\"conference\", \"\")).strip()\n",
    "    return conf.startswith(\"DFRWS\")\n",
    "\n",
    "\n",
    "# Load\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'results_combined_prompts_new.csv' not found.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Parse columns expected to be JSON blobs\n",
    "df[\"school_names_parsed\"] = df[\"school/organization_names\"].apply(safe_parse_json)\n",
    "df[\"authors_parsed\"]      = df[\"authors\"].apply(safe_parse_json)\n",
    "df[\"countries_parsed\"]    = df[\"author_countries\"].apply(safe_parse_json)\n",
    "df[\"year_parsed\"]         = df[\"published_year\"].apply(safe_parse_json)\n",
    "df[\"conference_parsed\"]   = df[\"conference\"].apply(safe_parse_json)\n",
    "\n",
    "# Filter: ALL DFRWS venues + year range (USA filter removed)\n",
    "mask = (\n",
    "    df[\"conference_parsed\"].apply(is_dfrws_any) &\n",
    "    df[\"year_parsed\"].apply(is_year_in_range)\n",
    ")\n",
    "filtered_df = df[mask].copy()\n",
    "\n",
    "# Counters\n",
    "school_counter     = Counter()\n",
    "author_counter     = Counter()\n",
    "country_counter    = Counter()\n",
    "conference_counter = Counter()   # e.g., DFRWS USA vs DFRWS EU, etc.\n",
    "\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # --- Conferences (string) ---\n",
    "    conf = str(row[\"conference_parsed\"].get(\"conference\", \"\")).strip()\n",
    "    if conf:\n",
    "        conference_counter[conf] += 1\n",
    "\n",
    "    # --- Schools (list) ---\n",
    "    schools = row[\"school_names_parsed\"].get(\"school_names\", [])\n",
    "    for school in schools:\n",
    "        s = str(school).strip()\n",
    "        if s:\n",
    "            school_counter[s] += 1\n",
    "\n",
    "    # --- Authors (comma-separated string inside JSON) ---\n",
    "    authors_raw = row[\"authors_parsed\"].get(\"authors\", \"\")\n",
    "    authors = [a.strip() for a in str(authors_raw).split(\",\") if a.strip()]\n",
    "    for author in authors:\n",
    "        author_counter[author] += 1\n",
    "\n",
    "    # --- Countries (list) ---\n",
    "    countries = row[\"countries_parsed\"].get(\"author_countries\", [])\n",
    "    for c in countries:\n",
    "        cc = str(c).strip()\n",
    "        if cc:\n",
    "            country_counter[cc] += 1\n",
    "\n",
    "# DataFrames\n",
    "top_schools_df = (\n",
    "    pd.DataFrame(school_counter.items(), columns=[\"School\", \"Count\"])\n",
    "    .sort_values(by=\"Count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "top_authors_df = (\n",
    "    pd.DataFrame(author_counter.items(), columns=[\"Author\", \"Count\"])\n",
    "    .sort_values(by=\"Count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "top_countries_df = (\n",
    "    pd.DataFrame(country_counter.items(), columns=[\"Country\", \"Count\"])\n",
    "    .sort_values(by=\"Count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "conference_df = (\n",
    "    pd.DataFrame(conference_counter.items(), columns=[\"Conference\", \"PaperCount\"])\n",
    "    .sort_values(by=\"PaperCount\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n=== DFRWS Venue Breakdown (2002–2025) ===\")\n",
    "for _, r in conference_df.iterrows():\n",
    "    print(f\"{r['Conference']}: {r['PaperCount']} papers\")\n",
    "\n",
    "print(\"\\n=== Top School Affiliations (DFRWS, 2002–2025) ===\")\n",
    "for _, r in top_schools_df.iterrows():\n",
    "    print(f\"{r['School']}: {r['Count']}\")\n",
    "\n",
    "print(\"\\n=== Most Frequent Authors (DFRWS, 2002–2025) ===\")\n",
    "for _, r in top_authors_df.iterrows():\n",
    "    print(f\"{r['Author']}: {r['Count']}\")\n",
    "\n",
    "print(\"\\n=== Countries in Author Affiliations (DFRWS, 2002–2025) ===\")\n",
    "for _, r in top_countries_df.iterrows():\n",
    "    print(f\"{r['Country']}: {r['Count']}\")\n",
    "\n",
    "print(f\"\\n📄 Total Number of Matching Papers: {len(filtered_df)}\")\n",
    "\n",
    "# Save CSVs (neutral filenames)\n",
    "top_schools_df.to_csv(\"top_schools_dfrws_2002_2025.csv\", index=False)\n",
    "top_authors_df.to_csv(\"top_authors_dfrws_2002_2025.csv\", index=False)\n",
    "top_countries_df.to_csv(\"top_countries_dfrws_2002_2025.csv\", index=False)\n",
    "conference_df.to_csv(\"dfrws_venue_breakdown_2002_2025.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf8d304-3cd9-4a37-b00c-f2629aaae416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Config\n",
    "csv_path = \"results_combined_prompts_new.csv\"  # Ensure this file is in the same folder\n",
    "year_min, year_max = 2001, 2025               # Inclusive range\n",
    "TOP_N = 10                                    # <-- Top 10 only\n",
    "\n",
    "# Helpers\n",
    "def safe_parse_json(cell):\n",
    "    \"\"\"\n",
    "    Parse cells that should contain JSON.\n",
    "    Strips code fences like ```json ... ``` if present.\n",
    "    Returns {} on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cleaned = re.sub(r\"```json|```\", \"\", str(cell)).strip()\n",
    "        return json.loads(cleaned)\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def is_year_in_range(entry, lo=year_min, hi=year_max):\n",
    "    try:\n",
    "        year = int(entry.get(\"year\", \"0\"))\n",
    "        return lo <= year <= hi\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def is_dfrws_any(entry):\n",
    "    \"\"\"\n",
    "    Match ALL DFRWS venues (USA, EU, APAC, etc.)\n",
    "    Expects parsed conference like {\"conference\": \"DFRWS USA\"}.\n",
    "    \"\"\"\n",
    "    conf = str(entry.get(\"conference\", \"\")).strip()\n",
    "    return conf.startswith(\"DFRWS\")\n",
    "\n",
    "# Load\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'results_combined_prompts_new.csv' not found.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Parse columns expected to be JSON blobs\n",
    "df[\"school_names_parsed\"] = df[\"school/organization_names\"].apply(safe_parse_json)\n",
    "df[\"authors_parsed\"]      = df[\"authors\"].apply(safe_parse_json)\n",
    "df[\"countries_parsed\"]    = df[\"author_countries\"].apply(safe_parse_json)\n",
    "df[\"year_parsed\"]         = df[\"published_year\"].apply(safe_parse_json)\n",
    "df[\"conference_parsed\"]   = df[\"conference\"].apply(safe_parse_json)\n",
    "\n",
    "# Filter: ALL DFRWS venues + year range (no USA-only filter)\n",
    "mask = (\n",
    "    df[\"conference_parsed\"].apply(is_dfrws_any) &\n",
    "    df[\"year_parsed\"].apply(is_year_in_range)\n",
    ")\n",
    "filtered_df = df[mask].copy()\n",
    "\n",
    "# Counters\n",
    "school_counter     = Counter()\n",
    "author_counter     = Counter()\n",
    "country_counter    = Counter()\n",
    "conference_counter = Counter()   # e.g., DFRWS USA vs DFRWS EU, etc.\n",
    "\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # --- Conferences (string) ---\n",
    "    conf = str(row[\"conference_parsed\"].get(\"conference\", \"\")).strip()\n",
    "    if conf:\n",
    "        conference_counter[conf] += 1\n",
    "\n",
    "    # --- Schools (list) ---\n",
    "    schools = row[\"school_names_parsed\"].get(\"school_names\", [])\n",
    "    for school in schools:\n",
    "        s = str(school).strip()\n",
    "        if s:\n",
    "            school_counter[s] += 1\n",
    "\n",
    "    # --- Authors (comma-separated string inside JSON) ---\n",
    "    authors_raw = row[\"authors_parsed\"].get(\"authors\", \"\")\n",
    "    authors = [a.strip() for a in str(authors_raw).split(\",\") if a.strip()]\n",
    "    for author in authors:\n",
    "        author_counter[author] += 1\n",
    "\n",
    "    # --- Countries (list) ---\n",
    "    countries = row[\"countries_parsed\"].get(\"author_countries\", [])\n",
    "    for c in countries:\n",
    "        cc = str(c).strip()\n",
    "        if cc:\n",
    "            country_counter[cc] += 1\n",
    "\n",
    "# DataFrames (full), then restrict to Top N\n",
    "schools_df_all = (\n",
    "    pd.DataFrame(school_counter.items(), columns=[\"School\", \"Count\"])\n",
    "    .sort_values(by=\"Count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "authors_df_all = (\n",
    "    pd.DataFrame(author_counter.items(), columns=[\"Author\", \"Count\"])\n",
    "    .sort_values(by=\"Count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "countries_df_all = (\n",
    "    pd.DataFrame(country_counter.items(), columns=[\"Country\", \"Count\"])\n",
    "    .sort_values(by=\"Count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "conference_df_all = (\n",
    "    pd.DataFrame(conference_counter.items(), columns=[\"Conference\", \"PaperCount\"])\n",
    "    .sort_values(by=\"PaperCount\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "top_schools_df   = schools_df_all.head(TOP_N)\n",
    "top_authors_df   = authors_df_all.head(TOP_N)\n",
    "top_countries_df = countries_df_all.head(TOP_N)\n",
    "top_conference_df = conference_df_all.head(TOP_N)\n",
    "\n",
    "# Output (formatted to avoid \"USA4\" run-ons)\n",
    "# \n",
    "print(f\"\\n=== DFRWS Venue Breakdown (2002–2025) — Top {TOP_N} ===\")\n",
    "for _, r in top_conference_df.iterrows():\n",
    "    print(f\"{r['Conference']}: {r['PaperCount']} papers\")\n",
    "\n",
    "print(f\"\\n=== Top {TOP_N} School Affiliations (DFRWS, 2002–2025) ===\")\n",
    "for _, r in top_schools_df.iterrows():\n",
    "    print(f\"{r['School']}: {r['Count']}\")\n",
    "\n",
    "print(f\"\\n=== Top {TOP_N} Most Frequent Authors (DFRWS, 2002–2025) ===\")\n",
    "for _, r in top_authors_df.iterrows():\n",
    "    print(f\"{r['Author']}: {r['Count']}\")\n",
    "\n",
    "print(f\"\\n=== Top {TOP_N} Countries in Author Affiliations (DFRWS, 2002–2025) ===\")\n",
    "for _, r in top_countries_df.iterrows():\n",
    "    print(f\"{r['Country']}: {r['Count']}\")\n",
    "\n",
    "print(f\"\\n Total Number of Matching Papers: {len(filtered_df)}\")\n",
    "\n",
    "# Save CSVs (Top N only)\n",
    "top_schools_df.to_csv(f\"top_{TOP_N}_schools_dfrws_2002_2025.csv\", index=False)\n",
    "top_authors_df.to_csv(f\"top_{TOP_N}_authors_dfrws_2002_2025.csv\", index=False)\n",
    "top_countries_df.to_csv(f\"top_{TOP_N}_countries_dfrws_2002_2025.csv\", index=False)\n",
    "top_conference_df.to_csv(f\"top_{TOP_N}_dfrws_venue_breakdown_2002_2025.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e7719-dfd9-450c-b9b2-7bbd010762a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DFRWS ANALYSIS — ONE GROUPED+STACKED GRAPH (Top-10 global countries + Other)\n",
    "+ prints/saves the per-year LONG table for LaTeX\n",
    "+ Top-10 tables + Per-conference Top-10 (with Other)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Config\n",
    "\n",
    "CSV_PATH = \"results_combined_prompts_new.csv\"\n",
    "OUTDIR = \"dfrws_outputs_grouped\"\n",
    "YEAR_MIN, YEAR_MAX = 2002, 2025\n",
    "\n",
    "TOP_N_LISTS = 10              # Top-N for schools/authors/countries tables\n",
    "TOP_N_COUNTRIES_GLOBAL = 10   # EXACTLY top-10 countries in stacks; others -> \"Other\"\n",
    "\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# Helpers\n",
    "\n",
    "def safe_parse_json(cell):\n",
    "    try:\n",
    "        cleaned = re.sub(r\"```json|```\", \"\", str(cell)).strip()\n",
    "        return json.loads(cleaned)\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def is_year_in_range(entry, lo=YEAR_MIN, hi=YEAR_MAX):\n",
    "    try:\n",
    "        year = int(entry.get(\"year\", \"0\"))\n",
    "        return lo <= year <= hi\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def is_dfrws_any(entry):\n",
    "    conf = str(entry.get(\"conference\", \"\")).strip()\n",
    "    return conf.startswith(\"DFRWS\")\n",
    "\n",
    "# Normalize country names to avoid splitting (UK vs United Kingdom, etc.)\n",
    "ALIASES = {\n",
    "    # USA\n",
    "    \"united states\": \"USA\", \"united states of america\": \"USA\",\n",
    "    \"u.s.\": \"USA\", \"u.s.a.\": \"USA\", \"us\": \"USA\", \"u.s\": \"USA\", \"usa\": \"USA\",\n",
    "    # UK\n",
    "    \"uk\": \"United Kingdom\", \"u.k.\": \"United Kingdom\",\n",
    "    \"england\": \"United Kingdom\", \"scotland\": \"United Kingdom\",\n",
    "    \"wales\": \"United Kingdom\", \"great britain\": \"United Kingdom\",\n",
    "    \"united kingdom\": \"United Kingdom\",\n",
    "    # South Korea\n",
    "    \"republic of korea\": \"South Korea\",\n",
    "    \"korea, republic of\": \"South Korea\",\n",
    "    \"korea (south)\": \"South Korea\",\n",
    "    # ISO-2 shortcuts & variants\n",
    "    \"de\": \"Germany\", \"gb\": \"United Kingdom\", \"kr\": \"South Korea\",\n",
    "    \"ch\": \"Switzerland\", \"ie\": \"Ireland\", \"ca\": \"Canada\",\n",
    "    \"au\": \"Australia\", \"nl\": \"Netherlands\",\n",
    "    \"the netherlands\": \"Netherlands\",\n",
    "    \"deutschland\": \"Germany\",\n",
    "}\n",
    "\n",
    "def canon_country(c):\n",
    "    key = str(c).strip().lower()\n",
    "    return ALIASES.get(key, str(c).strip())\n",
    "\n",
    "def sort_confs(confs):\n",
    "    def key_fn(s):\n",
    "        s_low = s.lower()\n",
    "        if \"usa\" in s_low:  return (0, s_low)\n",
    "        if \"eu\" in s_low:   return (1, s_low)\n",
    "        if \"apac\" in s_low: return (2, s_low)\n",
    "        return (3, s_low)\n",
    "    return sorted(confs, key=key_fn)\n",
    "\n",
    "def _safe_name(s):\n",
    "    return re.sub(r\"[^A-Za-z0-9_.-]+\", \"_\", str(s))\n",
    "\n",
    "\n",
    "# Load & Parse\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "except FileNotFoundError:\n",
    "    raise SystemExit(f\"ERROR: '{CSV_PATH}' not found.\")\n",
    "\n",
    "df[\"school_names_parsed\"] = df[\"school/organization_names\"].apply(safe_parse_json)\n",
    "df[\"authors_parsed\"]      = df[\"authors\"].apply(safe_parse_json)\n",
    "df[\"countries_parsed\"]    = df[\"author_countries\"].apply(safe_parse_json)\n",
    "df[\"year_parsed\"]         = df[\"published_year\"].apply(safe_parse_json)\n",
    "df[\"conference_parsed\"]   = df[\"conference\"].apply(safe_parse_json)\n",
    "\n",
    "mask = (\n",
    "    df[\"conference_parsed\"].apply(is_dfrws_any) &\n",
    "    df[\"year_parsed\"].apply(is_year_in_range)\n",
    ")\n",
    "filtered = df[mask].copy()\n",
    "\n",
    "\n",
    "# Top lists + raw records\n",
    "\n",
    "school_counter  = Counter()\n",
    "author_counter  = Counter()\n",
    "country_counter = Counter()\n",
    "\n",
    "records = []  # (year, conference, country)\n",
    "\n",
    "for _, row in filtered.iterrows():\n",
    "    # schools\n",
    "    for s in row[\"school_names_parsed\"].get(\"school_names\", []):\n",
    "        s = str(s).strip()\n",
    "        if s:\n",
    "            school_counter[s] += 1\n",
    "    # authors\n",
    "    authors_raw = row[\"authors_parsed\"].get(\"authors\", \"\")\n",
    "    for a in [x.strip() for x in str(authors_raw).split(\",\") if x.strip()]:\n",
    "        author_counter[a] += 1\n",
    "    # countries (+ capture conf/year)\n",
    "    year = row[\"year_parsed\"].get(\"year\", None)\n",
    "    try:\n",
    "        year = int(year)\n",
    "    except Exception:\n",
    "        year = None\n",
    "    conf = str(row[\"conference_parsed\"].get(\"conference\", \"\")).strip()\n",
    "    for c in row[\"countries_parsed\"].get(\"author_countries\", []):\n",
    "        cc = canon_country(c)\n",
    "        if cc:\n",
    "            country_counter[cc] += 1\n",
    "            if year is not None and conf:\n",
    "                records.append((year, conf, cc))\n",
    "\n",
    "# Save Top-10 CSVs\n",
    "pd.DataFrame(school_counter.items(), columns=[\"School\",\"Count\"]) \\\n",
    "  .sort_values(\"Count\", ascending=False).head(TOP_N_LISTS) \\\n",
    "  .to_csv(os.path.join(OUTDIR, f\"top_{TOP_N_LISTS}_schools_dfrws_{YEAR_MIN}_{YEAR_MAX}.csv\"), index=False)\n",
    "\n",
    "pd.DataFrame(author_counter.items(), columns=[\"Author\",\"Count\"]) \\\n",
    "  .sort_values(\"Count\", ascending=False).head(TOP_N_LISTS) \\\n",
    "  .to_csv(os.path.join(OUTDIR, f\"top_{TOP_N_LISTS}_authors_dfrws_{YEAR_MIN}_{YEAR_MAX}.csv\"), index=False)\n",
    "\n",
    "pd.DataFrame(country_counter.items(), columns=[\"Country\",\"Count\"]) \\\n",
    "  .sort_values(\"Count\", ascending=False).head(TOP_N_LISTS) \\\n",
    "  .to_csv(os.path.join(OUTDIR, f\"top_{TOP_N_LISTS}_countries_dfrws_{YEAR_MIN}_{YEAR_MAX}.csv\"), index=False)\n",
    "\n",
    "print(f\"Saved Top-{TOP_N_LISTS} tables to: {OUTDIR}\")\n",
    "\n",
    "# Build counts for grouped+stacked chart\n",
    "\n",
    "if not records:\n",
    "    raise SystemExit(\"No (year, conference, country) records found after filtering.\")\n",
    "\n",
    "counts = pd.DataFrame(records, columns=[\"year\",\"conference\",\"country\"]) \\\n",
    "           .groupby([\"year\",\"conference\",\"country\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "years = sorted([y for y in counts[\"year\"].unique() if YEAR_MIN <= y <= YEAR_MAX])\n",
    "confs = sort_confs(counts[\"conference\"].unique())\n",
    "\n",
    "# ----- EXACTLY Top-10 global countries; rest = \"Other\" -----\n",
    "global_totals = counts.groupby(\"country\")[\"count\"].sum().sort_values(ascending=False)\n",
    "named_countries = list(global_totals.head(TOP_N_COUNTRIES_GLOBAL).index)\n",
    "\n",
    "counts[\"country\"] = counts[\"country\"].where(counts[\"country\"].isin(named_countries), other=\"Other\")\n",
    "counts = counts.groupby([\"year\",\"conference\",\"country\"])[\"count\"].sum().reset_index()\n",
    "\n",
    "stack_cols = named_countries + ([\"Other\"] if \"Other\" in counts[\"country\"].values else [])\n",
    "\n",
    "# Build per-conference pivot with consistent columns\n",
    "by_conf = {}\n",
    "for conf in confs:\n",
    "    pivot = counts[counts[\"conference\"] == conf].pivot(index=\"year\", columns=\"country\", values=\"count\")\n",
    "    pivot = pivot.reindex(years).fillna(0.0)\n",
    "    for c in stack_cols:\n",
    "        if c not in pivot.columns:\n",
    "            pivot[c] = 0.0\n",
    "    pivot = pivot[stack_cols]\n",
    "    by_conf[conf] = pivot\n",
    "\n",
    "# -------------------------\n",
    "# SAVE & PRINT the per-year data for LaTeX\n",
    "# -------------------------\n",
    "counts_long = counts.sort_values([\"year\", \"conference\", \"country\"]).copy()\n",
    "counts_long_path = os.path.join(OUTDIR, f\"counts_top10_other_LONG_{YEAR_MIN}_{YEAR_MAX}.csv\")\n",
    "counts_long.to_csv(counts_long_path, index=False)\n",
    "\n",
    "for conf, pivot in by_conf.items():\n",
    "    out = pivot.reset_index()  # columns: year + each of the 10 countries + Other\n",
    "    out_path = os.path.join(OUTDIR, f\"{_safe_name(conf)}_year_by_country_top10_other_{YEAR_MIN}_{YEAR_MAX}.csv\")\n",
    "    out.to_csv(out_path, index=False)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", 1000)\n",
    "print(\"\\n DATA (per-year, after Top-10 + Other mapping) \")\n",
    "print(counts_long.to_string(index=False))\n",
    "print(\"\\n[INFO] Country stack order (legend order in the figure):\")\n",
    "print(\", \".join(stack_cols))\n",
    "print(f\"\\n[WROTE] {counts_long_path}\")\n",
    "for conf in by_conf:\n",
    "    print(f\"[WROTE] {os.path.join(OUTDIR, f'{_safe_name(conf)}_year_by_country_top10_other_{YEAR_MIN}_{YEAR_MAX}.csv')}\")\n",
    "\n",
    "# Plot: ONE grouped+stacked figure\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))  # single plot; default matplotlib colors\n",
    "\n",
    "n_conf = len(confs)\n",
    "group_width = 0.8\n",
    "bar_w = group_width / max(n_conf, 1)\n",
    "\n",
    "x = np.arange(len(years))  # year positions\n",
    "country_handles = []\n",
    "\n",
    "HATCHES = [\"\", \"///\", \"\\\\\\\\\", \"xx\", \"...\", \"++\", \"oo\", \"**\", \"||\", \"--\"]\n",
    "\n",
    "for i, conf in enumerate(confs):\n",
    "    x_offset = x - (group_width/2) + (i + 0.5)*bar_w\n",
    "    bottom = np.zeros(len(years))\n",
    "    first_conf_for_country_legend = (i == 0)\n",
    "\n",
    "    for j, c in enumerate(stack_cols):\n",
    "        vals = by_conf[conf][c].values\n",
    "        bars = ax.bar(\n",
    "            x_offset, vals, bottom=bottom, width=bar_w,\n",
    "            label=(c if first_conf_for_country_legend else None),\n",
    "            hatch=HATCHES[i % len(HATCHES)], edgecolor=\"black\"\n",
    "        )\n",
    "        if first_conf_for_country_legend and j == 0:\n",
    "            country_handles = []\n",
    "        if first_conf_for_country_legend:\n",
    "            country_handles.append(bars[0])\n",
    "        bottom = bottom + vals\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(years, rotation=45, ha=\"right\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(f\"DFRWS Authors by Country per Year (Grouped by Conference; Top {TOP_N_COUNTRIES_GLOBAL} + Other)\")\n",
    "\n",
    "# Legends\n",
    "country_labels = stack_cols\n",
    "leg1 = ax.legend(country_handles, country_labels, loc=\"upper center\",\n",
    "                 bbox_to_anchor=(0.5, -0.12), ncol=min(6, len(country_labels)),\n",
    "                 frameon=False, fontsize=\"small\")\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "conf_patches = [Patch(facecolor=\"white\", edgecolor=\"black\",\n",
    "                      hatch=HATCHES[i % len(HATCHES)], label=conf)\n",
    "                for i, conf in enumerate(confs)]\n",
    "leg2 = ax.legend(handles=conf_patches, loc=\"upper left\", bbox_to_anchor=(0.0, 1.02),\n",
    "                 ncol=min(4, len(conf_patches)), frameon=False, fontsize=\"small\")\n",
    "ax.add_artist(leg1)\n",
    "\n",
    "fig.tight_layout()\n",
    "one_graph_path = os.path.join(\n",
    "    OUTDIR,\n",
    "    f\"grouped_stacked_countries_by_year_top{TOP_N_COUNTRIES_GLOBAL}_{YEAR_MIN}_{YEAR_MAX}.png\"\n",
    ")\n",
    "fig.savefig(one_graph_path, dpi=300)\n",
    "plt.close(fig)\n",
    "print(f\"[OK] Saved ONE grouped+stacked figure: {one_graph_path}\")\n",
    "\n",
    "\n",
    "# ALL conferences: Top-10 countries tables (LONG + WIDE) with \"Other\"\n",
    "\n",
    "raw_counts = pd.DataFrame(records, columns=[\"year\",\"conference\",\"country\"])\n",
    "totals_by_conf_country = raw_counts.groupby([\"conference\", \"country\"]).size().reset_index(name=\"total_count\")\n",
    "\n",
    "per_conf_rows = []\n",
    "for conf, g in totals_by_conf_country.groupby(\"conference\"):\n",
    "    g = g.sort_values(\"total_count\", ascending=False).reset_index(drop=True)\n",
    "    named = g.head(TOP_N_COUNTRIES_GLOBAL).copy()\n",
    "    other_count = int(g[\"total_count\"].iloc[TOP_N_COUNTRIES_GLOBAL:].sum()) if len(g) > TOP_N_COUNTRIES_GLOBAL else 0\n",
    "    if other_count > 0:\n",
    "        named = pd.concat([named, pd.DataFrame([{\"conference\": conf, \"country\": \"Other\", \"total_count\": other_count}])],\n",
    "                          ignore_index=True)\n",
    "    total_conf = int(g[\"total_count\"].sum())\n",
    "    named[\"rank\"] = range(1, len(named) + 1)\n",
    "    named[\"share\"] = (named[\"total_count\"] / total_conf).round(4) if total_conf > 0 else 0.0\n",
    "    per_conf_rows.append(named)\n",
    "\n",
    "top_long = pd.concat(per_conf_rows, ignore_index=True).sort_values([\"conference\", \"rank\"])\n",
    "\n",
    "long_path = os.path.join(OUTDIR, f\"top_countries_per_conference_LONG_top{TOP_N_COUNTRIES_GLOBAL}_{YEAR_MIN}_{YEAR_MAX}.csv\")\n",
    "top_long.to_csv(long_path, index=False)\n",
    "\n",
    "rows = []\n",
    "for conf, g in top_long.groupby(\"conference\"):\n",
    "    g = g.sort_values(\"rank\")\n",
    "    row = {\"conference\": conf}\n",
    "    for i, r in enumerate(g.itertuples(index=False), start=1):\n",
    "        row[f\"country_{i}\"] = r.country\n",
    "        row[f\"count_{i}\"]   = int(r.total_count)\n",
    "        row[f\"share_{i}\"]   = float(r.share)\n",
    "    rows.append(row)\n",
    "top_wide = pd.DataFrame(rows)\n",
    "\n",
    "wide_path = os.path.join(OUTDIR, f\"top_countries_per_conference_WIDE_top{TOP_N_COUNTRIES_GLOBAL}_{YEAR_MIN}_{YEAR_MAX}.csv\")\n",
    "top_wide.to_csv(wide_path, index=False)\n",
    "\n",
    "print(f\"[OK] Saved per-conference Top-{TOP_N_COUNTRIES_GLOBAL} country tables (LONG): {long_path}\")\n",
    "print(f\"[OK] Saved per-conference Top-{TOP_N_COUNTRIES_GLOBAL} country tables (WIDE): {wide_path}\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f6b35-76c4-44e8-9a49-071511eac429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# ======== CONFIG ========\n",
    "CSV_PATH = \"results_combined_prompts_new.csv\"   # your file\n",
    "TARGET_COL = \"forensic_vs_anti\"\n",
    "# ========================\n",
    "\n",
    "# Robust read: let pandas infer comma/tab, handle quoted newlines\n",
    "df = pd.read_csv(\n",
    "    CSV_PATH,\n",
    "    sep=None,                # auto-detect delimiter\n",
    "    engine=\"python\",\n",
    "    dtype=str,\n",
    "    keep_default_na=False    # keep empty strings instead of NaN\n",
    ")\n",
    "\n",
    "# Clean column names only for safety (preserve spaces like \"Paper Title\")\n",
    "df.columns = [c.replace(\"\\u00A0\", \" \").strip() for c in df.columns]\n",
    "\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise KeyError(f\"Column '{TARGET_COL}' not found. Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# --- helpers ---\n",
    "CODE_FENCE_RE = re.compile(r\"^\\s*```(?:json)?\\s*|\\s*```\\s*$\", re.IGNORECASE)\n",
    "\n",
    "def strip_code_fences(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    return CODE_FENCE_RE.sub(\"\", s).strip()\n",
    "\n",
    "def parse_json_cell(cell):\n",
    "    raw = strip_code_fences(cell)\n",
    "    if not raw:\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def normalize_forensic_type(val):\n",
    "    if not val:\n",
    "        return None\n",
    "    v = str(val).strip().lower()\n",
    "    if v == \"forensic\" or v in {\"digital forensic\", \"digital forensics\"}:\n",
    "        return \"Digital Forensic\"\n",
    "    if v in {\"anti-forensic\", \"antiforensic\", \"anti forensic\"}:\n",
    "        return \"Anti-Forensic\"\n",
    "    return None  # treat anything else as unparseable/unknown\n",
    "\n",
    "# --- parse + count ---\n",
    "types = (\n",
    "    df[TARGET_COL]\n",
    "      .apply(parse_json_cell)\n",
    "      .apply(lambda d: normalize_forensic_type(d.get(\"forensic_type\")))\n",
    ")\n",
    "\n",
    "counts = Counter(t for t in types if t)\n",
    "\n",
    "# Print EXACTLY what you asked for\n",
    "print(\"Digital Forensic:\", counts.get(\"Digital Forensic\", 0))\n",
    "print(\"Anti-Forensic:   \", counts.get(\"Anti-Forensic\", 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587edb1-790c-4aad-aa86-57b701d1dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import Counter  # (not strictly needed, but ok to keep)\n",
    "import os\n",
    "\n",
    "# ========= CONFIG =========\n",
    "CSV_PATH = \"results_combined_prompts_new.csv\"\n",
    "OUTDIR = \"dfrws_outputs_dfvsaf_trends\"\n",
    "FILTER_DFRWS_ONLY = True   # set False if you want all venues\n",
    "INCLUDE_UNKNOWN = False    # set True to keep unknown/missing DF/AF labels\n",
    "MAKE_PLOT = False          # set True to also save a simple matplotlib plot\n",
    "# =========================\n",
    "\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# --- helpers (same pattern as before) ---\n",
    "CODE_FENCE_RE = re.compile(r\"^\\s*```(?:json)?\\s*|\\s*```\\s*$\", re.IGNORECASE)\n",
    "\n",
    "def strip_code_fences(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    return CODE_FENCE_RE.sub(\"\", s).strip()\n",
    "\n",
    "def parse_json_cell(cell):\n",
    "    raw = strip_code_fences(cell)\n",
    "    if not raw:\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def normalize_forensic_type(val):\n",
    "    if not val:\n",
    "        return None\n",
    "    v = str(val).strip().lower()\n",
    "    if v == \"forensic\" or v in {\"digital forensic\", \"digital forensics\"}:\n",
    "        return \"Digital Forensic\"\n",
    "    if v in {\"anti-forensic\", \"antiforensic\", \"anti forensic\"}:\n",
    "        return \"Anti-Forensic\"\n",
    "    return None  # treat anything else as unknown\n",
    "\n",
    "def parse_year(cell):\n",
    "    d = parse_json_cell(cell)\n",
    "    y = d.get(\"year\")\n",
    "    try:\n",
    "        return int(y)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def parse_conf(cell):\n",
    "    d = parse_json_cell(cell)\n",
    "    return str(d.get(\"conference\", \"\")).strip()\n",
    "\n",
    "# --- load (let pandas infer delimiter; handles quoted newlines) ---\n",
    "df = pd.read_csv(\n",
    "    CSV_PATH, sep=None, engine=\"python\", dtype=str, keep_default_na=False\n",
    ")\n",
    "df.columns = [c.replace(\"\\u00A0\", \" \").strip() for c in df.columns]\n",
    "\n",
    "need_cols = [\"published_year\", \"forensic_vs_anti\", \"conference\"]\n",
    "for c in need_cols:\n",
    "    if c not in df.columns:\n",
    "        raise KeyError(f\"Missing column '{c}'. Found: {df.columns.tolist()}\")\n",
    "\n",
    "# --- parse needed fields ---\n",
    "df[\"year\"] = df[\"published_year\"].apply(parse_year)\n",
    "df[\"type\"] = df[\"forensic_vs_anti\"].apply(lambda s: normalize_forensic_type(parse_json_cell(s).get(\"forensic_type\")))\n",
    "df[\"conf\"] = df[\"conference\"].apply(parse_conf)\n",
    "\n",
    "# optional filter: only DFRWS venues\n",
    "if FILTER_DFRWS_ONLY:\n",
    "    df = df[df[\"conf\"].str.startswith(\"DFRWS\", na=False)].copy()\n",
    "\n",
    "if not INCLUDE_UNKNOWN:\n",
    "    df = df[df[\"type\"].isin([\"Digital Forensic\", \"Anti-Forensic\"])].copy()\n",
    "\n",
    "df = df[df[\"year\"].notna()].copy()\n",
    "\n",
    "if df.empty:\n",
    "    raise SystemExit(\"No rows after filtering/parsing.\")\n",
    "\n",
    "# ---------- LONG table: year × type ----------\n",
    "long = (\n",
    "    df.groupby([\"year\", \"type\"])\n",
    "      .size().reset_index(name=\"count\")\n",
    "      .sort_values([\"year\", \"type\"])\n",
    ")\n",
    "long_path = os.path.join(OUTDIR, \"dfvsaf_by_year_LONG.csv\")\n",
    "long.to_csv(long_path, index=False)\n",
    "\n",
    "# ---------- WIDE table (with shares) ----------\n",
    "wide = long.pivot(index=\"year\", columns=\"type\", values=\"count\").fillna(0).astype(int)\n",
    "for col in [\"Digital Forensic\", \"Anti-Forensic\"]:\n",
    "    if col not in wide.columns:\n",
    "        wide[col] = 0\n",
    "wide = wide[[\"Digital Forensic\", \"Anti-Forensic\"]]\n",
    "wide[\"Total\"] = wide.sum(axis=1)\n",
    "wide[\"Digital_Forensic_share\"] = (wide[\"Digital Forensic\"] / wide[\"Total\"]).fillna(0).round(4)\n",
    "wide[\"Anti-Forensic_share\"] = (wide[\"Anti-Forensic\"] / wide[\"Total\"]).fillna(0).round(4)\n",
    "wide_path = os.path.join(OUTDIR, \"dfvsaf_by_year_WIDE.csv\")\n",
    "wide.reset_index().to_csv(wide_path, index=False)\n",
    "\n",
    "# ---------- print to console ----------\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "print(\"\\n[DF vs AF by Year — LONG]\")\n",
    "print(long.to_string(index=False))\n",
    "print(\"\\n[DF vs AF by Year — WIDE]\")\n",
    "print(wide.reset_index().to_string(index=False))\n",
    "print(f\"\\n[WROTE] {long_path}\")\n",
    "print(f\"[WROTE] {wide_path}\")\n",
    "\n",
    "# ---------- optional plot ----------\n",
    "if MAKE_PLOT:\n",
    "    import matplotlib.pyplot as plt  # no seaborn\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    years = wide.index.tolist()\n",
    "    ax.plot(years, wide[\"Digital Forensic\"].tolist(), label=\"Digital Forensic\")\n",
    "    ax.plot(years, wide[\"Anti-Forensic\"].tolist(), label=\"Anti-Forensic\")\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(\"DF vs AF Counts by Year\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plot_path = os.path.join(OUTDIR, \"dfvsaf_by_year_lines.png\")\n",
    "    fig.savefig(plot_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(f\"[WROTE] {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef25a6b8-3216-4bfa-9a45-8dfd16e07f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json, re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CSV_PATH = \"results_combined_prompts_new.csv\"  \n",
    "\n",
    "# --- helpers ---\n",
    "CODE_FENCE_RE = re.compile(r\"^\\s*```(?:json)?\\s*|\\s*```\\s*$\", re.IGNORECASE)\n",
    "def strip_code_fences(s): return CODE_FENCE_RE.sub(\"\", str(s)).strip()\n",
    "\n",
    "def parse_json(cell):\n",
    "    raw = strip_code_fences(cell)\n",
    "    try: return json.loads(raw) if raw else {}\n",
    "    except Exception: return {}\n",
    "\n",
    "def norm_type(v):\n",
    "    if not v: return None\n",
    "    v = str(v).strip().lower()\n",
    "    if v == \"forensic\" or v in {\"digital forensic\",\"digital forensics\"}:\n",
    "        return \"Digital Forensic\"\n",
    "    if v in {\"anti-forensic\",\"antiforensic\",\"anti forensic\"}:\n",
    "        return \"Anti-Forensic\"\n",
    "    return None\n",
    "\n",
    "def parse_year(cell):\n",
    "    y = parse_json(cell).get(\"year\")\n",
    "    try: return int(y)\n",
    "    except Exception: return None\n",
    "\n",
    "def parse_conf(cell):\n",
    "    return str(parse_json(cell).get(\"conference\",\"\")).strip()\n",
    "\n",
    "# --- load & parse ---\n",
    "df = pd.read_csv(CSV_PATH, sep=None, engine=\"python\", dtype=str, keep_default_na=False)\n",
    "df.columns = [c.replace(\"\\u00A0\",\" \").strip() for c in df.columns]\n",
    "\n",
    "df[\"year\"] = df[\"published_year\"].apply(parse_year)\n",
    "df[\"type\"] = df[\"forensic_vs_anti\"].apply(lambda s: norm_type(parse_json(s).get(\"forensic_type\")))\n",
    "df[\"conf\"] = df[\"conference\"].apply(parse_conf)\n",
    "\n",
    "# DFRWS only + drop unknowns/invalid years\n",
    "df = df[df[\"conf\"].str.startswith(\"DFRWS\", na=False)]\n",
    "df = df[df[\"type\"].isin([\"Digital Forensic\",\"Anti-Forensic\"])]\n",
    "df = df[df[\"year\"].notna()]\n",
    "\n",
    "# --- LONG table (year, type, count) ---\n",
    "df_long = (\n",
    "    df.groupby([\"year\",\"type\"])\n",
    "      .size().reset_index(name=\"count\")\n",
    "      .sort_values([\"year\",\"type\"])\n",
    ")\n",
    "\n",
    "# --- WIDE for plotting ---\n",
    "wide = df_long.pivot(index=\"year\", columns=\"type\", values=\"count\").fillna(0).astype(int)\n",
    "for col in [\"Digital Forensic\",\"Anti-Forensic\"]:\n",
    "    if col not in wide.columns: wide[col] = 0\n",
    "wide = wide.reindex(sorted(wide.index))\n",
    "years = wide.index.astype(int).tolist()\n",
    "dfc = wide[\"Digital Forensic\"].to_numpy()\n",
    "afc = wide[\"Anti-Forensic\"].to_numpy()\n",
    "\n",
    "# --------- Figure A: Grouped bars (counts) ---------\n",
    "x = range(len(years)); w = 0.42\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.bar([i - w/2 for i in x], dfc, width=w, label=\"Digital Forensic\")\n",
    "ax.bar([i + w/2 for i in x], afc, width=w, label=\"Anti-Forensic\")\n",
    "\n",
    "# label tiny AF bars\n",
    "for i, v in enumerate(afc):\n",
    "    if v > 0:\n",
    "        ax.text(i + w/2, v + 0.2, str(v), ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "ax.set_xticks(list(x))\n",
    "ax.set_xticklabels(years, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"Paper count\")\n",
    "ax.set_title(\"DF vs AF papers by year (DFRWS)\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --------- Figure B (optional): 100% stacked (shares) ---------\n",
    "wide[\"Total\"] = (wide[\"Digital Forensic\"] + wide[\"Anti-Forensic\"]).replace(0, 1)\n",
    "share_df = wide[[\"Digital Forensic\",\"Anti-Forensic\"]].div(wide[\"Total\"], axis=0)\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(12,5))\n",
    "ax2.bar(years, share_df[\"Digital Forensic\"].values, label=\"Digital Forensic\")\n",
    "ax2.bar(years, share_df[\"Anti-Forensic\"].values,\n",
    "        bottom=share_df[\"Digital Forensic\"].values, label=\"Anti-Forensic\")\n",
    "ax2.set_xticks(list(range(len(years))))\n",
    "ax2.set_xticklabels(years, rotation=45, ha=\"right\")\n",
    "ax2.set_ylabel(\"Share\")\n",
    "ax2.set_title(\"DF vs AF shares by year (DFRWS)\")\n",
    "ax2.legend()\n",
    "fig2.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf83aad-0f00-4cf6-ac15-243ea49a62b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
